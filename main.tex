\documentclass[journal]{IEEEtran}

\usepackage{adjustbox}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{bookmark}
\usepackage[american]{circuitikz}
\usepackage{cite}
\usepackage{fixmath}
\usepackage[acronym]{glossaries-extra}
\usepackage{hyperref}
\usepackage{import}
\usepackage{mathtools}
\usepackage{microtype}
\usepackage[short]{optidef}
\usepackage{pgfplots}
\usepackage[subtle]{savetrees}
\usepackage{siunitx}
\usepackage{stfloats}
\usepackage[caption=false,font=footnotesize,subrefformat=parens,labelformat=parens]{subfig}
\usepackage{tikz}
\usepackage{xcolor}

% dark mode
\usepackage{xcolor} \pagecolor[rgb]{0,0,0} \color[rgb]{1,1,1}

% amsthm
\newtheorem{proposition}{Proposition}
\newtheorem{remark}{Remark}

% siunitx
\DeclareSIUnit{\belm}{Bm}
\DeclareSIUnit{\dBm}{\deci\belm}
\DeclareSIUnit{\beli}{Bi}
\DeclareSIUnit{\dBi}{\deci\beli}

% PGF/TikZ
\usetikzlibrary{arrows,calc,matrix,patterns,plotmarks,positioning}
\usepgfplotslibrary{groupplots,patchplots}
\pgfplotsset{compat=newest}

% algpseudocode
\makeatletter
\renewcommand{\fnum@algorithm}{\fname@algorithm{} \thealgorithm:}
\makeatother
\algrenewcommand{\algorithmicrequire}{\textbf{Input:}}
\algrenewcommand{\algorithmicensure}{\textbf{Output:}}
\algrenewcommand{\algorithmicwhile}{\textbf{While}}
\algrenewcommand{\algorithmicend}{\textbf{End}}
\algrenewcommand{\algorithmicrepeat}{\textbf{Repeat}}
\algrenewcommand{\algorithmicuntil}{\textbf{Until}}
\algrenewcommand{\algorithmicdo}{}

% glossaries-extra
\setabbreviationstyle[acronym]{long-short}
\newacronym{af}{AF}{Amplify-and-Forward}
\newacronym{ambc}{AmBC}{Ambient Backscatter Communications}
\newacronym{ap}{AP}{Access Point}
\newacronym{awgn}{AWGN}{Additive White Gaussian Noise}
\newacronym{bcd}{BCD}{Block Coordinate Descent}
\newacronym{bc}{BackCom}{Backscatter Communications}
\newacronym{bibo}{BIBO}{Binary-Input Binary-Output}
\newacronym{bpcu}{\si{bpcu}}{bits per channel use}
\newacronym{bpsphz}{\si{bps/Hz}}{bits per second per Hertz}
\newacronym{cp}{CP}{Canonical Polyadic}
\newacronym{cr}{CR}{Cognitive Radio}
\newacronym{cscg}{CSCG}{Circularly Symmetric Complex Gaussian}
\newacronym{csi}{CSI}{Channel State Information}
\newacronym{df}{DF}{Decode-and-Forward}
\newacronym{dmc}{DMC}{Discrete Memoryless Channel}
\newacronym{dmtc}{DMTC}{Discrete Memoryless Thresholding Channel}
\newacronym{dmtmac}{DMTMAC}{Discrete Memoryless Thresholding Multiple Access Channel}
\newacronym{dtmac}{DTMAC}{Discrete Thresholding Multiple Access Channel}
\newacronym{dp}{DP}{Dynamic Programming}
\newacronym{fdma}{FDMA}{Frequency-Division Multiple Access}
\newacronym{iid}{i.i.d.}{independent and identically distributed}
\newacronym{ioe}{IoE}{Internet of Everything}
\newacronym{iot}{IoT}{Internet of Things}
\newacronym{kkt}{KKT}{Karush-Kuhn-Tucker}
\newacronym{m2m}{M2M}{Machine to Machine}
\newacronym{mac}{MAC}{Multiple Access Channel}
\newacronym{mc}{MC}{Multiplication Coding}
\newacronym{miso}{MISO}{Multiple-Input Single-Output}
\newacronym{mimo}{MIMO}{Multiple-Input Multiple-Output}
\newacronym{ml}{ML}{Maximum-Likelihood}
\newacronym{noma}{NOMA}{Non-Orthogonal Multiple Access}
\newacronym{ofdm}{OFDM}{Orthogonal Frequency-Division Multiplexing}
\newacronym{pdf}{PDF}{Probability Density Function}
\newacronym{pgd}{PGD}{Projected Gradient Descent}
\newacronym{psk}{PSK}{Phase Shift Keying}
\newacronym{qam}{QAM}{Quadrature Amplitude Modulation}
\newacronym{qos}{QoS}{Quality of Service}
\newacronym{rf}{RF}{Radio-Frequency}
\newacronym{rfid}{RFID}{Radio-Frequency Identification}
\newacronym{ris}{RIS}{Reconfigurable Intelligent Surface}
\newacronym{sc}{SC}{Superposition Coding}
\newacronym{sic}{SIC}{Successive Interference Cancellation}
\newacronym{simo}{SIMO}{Single-Input Multiple-Output}
\newacronym{sinr}{SINR}{Signal-to-Interference-plus-Noise Ratio}
\newacronym{smawk}{SMAWK}{Shor-Moran-Aggarwal-Wilber-Klawe}
\newacronym{snr}{SNR}{Signal-to-Noise Ratio}
\newacronym{sr}{SR}{Symbiotic Radio}
\newacronym{tdma}{TDMA}{Time-Division Multiple Access}
\newacronym{ue}{UE}{user}
\newacronym{wit}{WIT}{Wireless Information Transfer}
\newacronym{wpcn}{WPCN}{Wireless Powered Communication Network}
\newacronym{wpt}{WPT}{Wireless Power Transfer}


\begin{document}
\title{UniScatter: Unifying\\Backscatter Communications, Symbiotic Radio, and Reconfigurable Intelligent Surface}
\author{
	\IEEEauthorblockN{
		Yang~Zhao,~\IEEEmembership{Member,~IEEE,}
		and~Bruno~Clerckx,~\IEEEmembership{Fellow,~IEEE}
	}
	\thanks{
		The authors are with the Department of Electrical and Electronic Engineering, Imperial College London, London SW7 2AZ, U.K. (e-mail: \{yang.zhao18, b.clerckx\}@imperial.ac.uk).
	}
}
\maketitle

\begin{abstract}
	Scatterers can harvest energy from, modulate information over, and influence propagation of surrounding radio waves.
	\gls{bc} varies object impedance to manipulate the magnitude, phase, and/or frequency of scattered signal to encode information and deliver within coverage.
	\gls{ris} adapts scattering antennas or programmable metamaterial to control wireless propagation environment by boosting/suppressing signal strength in specific directions.
	\gls{sr} incorporates scatter nodes into active networks that recycle ambient signal to transmit self information and enhance legacy channel to the cooperative receiver.
	In this paper, we depart from those concepts and introduce UniScatter as a new paradigm for future wireless networks.
	Instead of treating probability distribution of reflection states as equiprobable (as scattering source of \gls{bc}/\gls{sr}) or degenerate (as reflecting element of \gls{ris}), UniScatter node adapts input distribution of a passive scatter node based on link priority and \gls{csi}, balancing information encoding and channel reconfiguration in a flexible and mutualistic manner.
	To accommodate signal characteristics, UniScatter receiver semi-coherently decodes all nodes from accumulated energy, acquires equivalent primary channel over reflection pattern, then coherently decodes the primary link under enhanced multipath.
	It reduces the complexity of cooperative decoding while preserves the benefits of backscatter modulation and passive beamforming.
	Using shared spectrum, energy, and infrastructures, UniScatter is a more general and powerful transmit-assist protocol that generalizes \gls{bc}, \gls{ris} and \gls{sr} with universal hardware design and augmented \gls{qos} control.
	We consider an application scenario where a multi-antenna \gls{ap} serves a single-antenna user surrounded by multiple UniScatter nodes, and characterize the achievable primary-(total-)backscatter rate region by designing input distribution at the nodes, active beamforming at the \gls{ap}, and backscatter decision design at the user.
	Simulation results demonstrate UniScatter nodes can flexibly control the transmit-assist tradeoff via smart input distribution design.
\end{abstract}

\glsresetall

\begin{section}{Introduction}
	% TODO You need to explain the key principle and benefits of backscatter, then symbiotic radio, then RIS. Then you need to explain about the apparent differences and similarities between those techniques. Then metascatter concept will appear.
	\IEEEPARstart{F}{uture} wireless network is envisioned to provide high throughput, uniform coverage, pervasive connectivity, heterogeneous control, and cognitive intelligence for trillions of portable devices.
	As an emerging low-power communication technique, \gls{bc} separates conventional transmitter into a \gls{rf} emitter with power-hungry elements (e.g., synthesizer and amplifier), and an information-bearing node with power-efficient components (e.g., harvester and modulator) \cite{Boyer2014}.
	The node harvests energy from incident wave and embeds information over scattered signal in a sustainable and controllable manner.
	The backscatter reader can be either co-located or separated with the emitter, known as monostatic and bistatic \gls{bc}.
	Its applications such as \gls{rfid} \cite{Dobkin2012,Landt2005} and sensor networks \cite{Vannucci2008,Assimonis2016} have been extensively researched, standardized, and commercialized to support \gls{iot} and \gls{m2m}.
	% Specifically, a dedicated emitter generates a sinusoidal carrier at reserved frequency, and \gls{rfid} tags reports their identifier to a nearby \gls{rfid} reader.
	% ? With the upsurge of wireless devices and the downtrend of circuit power consumption, \gls{rfid} has experienced great success in \gls{ioe} and \gls{m2m} networks.
	However, traditional \gls{bc} requires dedicated carrier emitter and backscatter reader, while passive nodes only respond when externally inquired.
	In \gls{ambc} \cite{Liu2013b}, interactive nodes recycle ambient signals generated by legacy transmitters (e.g., radio, television, and Wi-Fi) to harvest energy and establish connection.
	It eliminates the need of dedicated power source, carrier emitter, and frequency spectrum, bringing more opportunities to low-power communications.
	To combat the strong direct-link interference of \gls{ambc}, \cite{Yang2018} proposed a co-located receiver that cooperatively decodes the primary (legacy) and backscatter links.
	The authors evaluated the error performance of \gls{ml}, linear, and \gls{sic} detectors for flat fading channel, and proposed a low-complexity detector for frequency-selective fading channel.
	The concept of cooperative \gls{ambc} was then refined as \gls{sr} to cognitively incorporate \gls{ambc} with existing systems \cite{Liang2020}.
	In \gls{sr}, the primary transmitter generates active radio carrying primary information, the secondary node modulates scattered component with backscatter information, and the cooperative receiver decodes both links from two propagation paths.
	The direct transmitter-receiver path only contains primary information, while the cascaded transmitter-node-receiver path preserves both information due to backscatter principles.
	Such a coexistence was further classified into commensal, parasitic, and competitive relationships based on link priority \cite{Guo2019b}, and their instantaneous rates, optimal power allocations, and outage probabilities were subsequently derived in \cite{Guo2019b,Ding2020}.
	However, one important issue of \gls{sr} is practical cooperative receiver design.
	Ideal joint \gls{ml} decoding achieves optimal performance with prohibitive computational complexity \cite{Yang2018,Zhang2022}.
	Due to physical constraints at the load-switching modulator, backscatter symbol period is typically longer than primary.
	For sequential decoding from primary to backscatter, \cite{Long2020a} pointed out the randomness from backscatter modulation can be modelled as either interference or channel uncertainty, depending on symbol period ratio.
	The authors concluded if this ratio is sufficiently large, the non-coherent primary achievable rate would asymptotically approach its coherent counterpart.
	This motivated \cite{Long2020a,Ding2020,Zhou2019a,Wu2021a,Xu2021a,Yang2021a,Yang2018,Han2021,Zhang2022} to decode the primary link, perform \gls{sic}, then decode the backscatter link.
	However, the advantage of \gls{sic} is questionable because 1) backscatter pattern and signal characteristics are not fully exploited; 2) primary rate analysis considers moderately long backscatter symbol period that constraints backscatter rate; 3) non-coherent primary encoding is required at the transmitter, while re-encoding, precoding, and subtraction are required at the receiver; 4) primary and backscatter symbols are mixed by multiplication instead of superposition.
	Another open issue for \gls{sr} is backscatter multiple access.
	\cite{Xu2021a} proposed a backscatter \gls{noma}-based \gls{sr} by extending \gls{sic} to multi-node scenario, where the decoding order depends on backscatter signal strength.
	However, its performance deteriorates fast when the number of nodes increases.
	Backscatter \gls{tdma} was also evaluated in \cite{Yang2021a}, where each node transmits information during its own slot and harvests energy during others.
	Although energy efficiency can be enhanced by transmission time and reflection ratio design, it requires regular feedback to passive nodes and incurs high coordination cost.
	\cite{Vougioukas2019} controls the load-switching speed at nodes to shift the scattered signal to desired frequency bands, which enables backscatter \gls{fdma} at the cost of extra bandwidth and higher power consumption.
	To reduce coordination between passive nodes, \cite{Han2021} proposed a random code-assisted multiple access for \gls{sr} and evaluated the asymptotic \gls{sinr} using random matrix theory.
	However, it suffers from imperfect synchronization and the near-far problem.
	% \cite{Zhou2019a} also explored the asymptotic impact of transmit/receive antenna and backscatter symbol period on the ergodic rate of primary and backscatter links.
	% For a \gls{mimo} \gls{sr} system with a multi-antenna backscatter node, \cite{Wu2021a} proposed a beamforming design to maximize the backscatter rate while guaranteeing the primary performance.
	% However, those paper only considered one backscatter node and backscatter multiple access remains an open issue.
	% In \cite{Xu2021a}, a \gls{noma}-based \gls{sr} was proposed and receive combining was investigated when \gls{sic} order follows equivalent channel strength.
	% A \gls{tdma}-based \gls{sr} with energy harvesting constraints was also presented in \cite{Yang2021a}, where transmit power, reflection efficiency, and time allocation were jointly optimized to maximize energy efficiency.
	% To reduce coordination between passive nodes, \cite{Han2021} proposed a random code-assisted multiple access for \gls{sr} and evaluated the asymptotic \gls{sinr} using random matrix theory.

	On the other hand, \gls{ris} is a promising technology that evolves wireless propagation environment using numerous passive reflecting elements (e.g., scattering antenna or programmable metamaterial) with adjustable amplitudes and/or phases \cite{Wu2021b}.
	The scattered signals contain no additional information, but adds constructively or destructively with the direct component to enhance desired signal or suppress interference.
	Compared with backscatter nodes of \gls{bc} and \gls{sr}, the reflection pattern of \gls{ris} elements are deterministic and priorly known at transmitter and receiver.
	It motivated the use of fixed reflection pattern during each channel block to improve communication, sensing, and power performances \cite{Wu2018,Zhang2019a,Lin2022,Liu2022,Feng2022,Zhao2022}.
	The concept of dynamic \gls{ris}, namely choosing independent reflection coefficients over different time slots within channel block, was first considered for resource blocks of \gls{ofdm} systems, then extended to power and information phases of \gls{wpcn} \cite{Wu2021,Wu2021d,Hua2022a}.
	Dynamic \gls{ris} provided artificial channel diversity and flexible resource allocation, but missed the opportunity to encode its own message.
	From an information-theoretic perspective, \cite{Karasik2020} reported using \gls{ris} as an auxiliary equipment to maximize the \gls{snr} is generally rate-suboptimal for finite input constellations.
	Instead, joint transmitter-\gls{ris} encoding achieves the capacity of \gls{ris}-aided channel, and layered encoding with \gls{sic} decoding (i.e., \gls{sic}-based \gls{sr}) can outperform pure passive beamforming at high \gls{snr}.
	It inspired \cite{Liu2019d,Bereyhi2020,Xu2020b,Zhang2021d,Hu2021b,Hua2022,Basar2020,Ma2020a,Yuan2021,Hu2021a} to employ \gls{ris} as an information source that combines passive beamforming and backscatter modulation in the overall reflection pattern.
	In particular, \emph{symbol level precoding} maps backscatter symbols to optimized \gls{ris} coefficient sets \cite{Liu2019d,Bereyhi2020}, \emph{overlay modulation} superposes information-bearing symbols over a common auxiliary matrix \cite{Xu2020b,Zhang2021d,Hu2021b,Hua2022}, \emph{spatial modulation} switches between reflection coefficient sets that maximize \gls{snr} at different receive antennas \cite{Basar2020,Ma2020a,Yuan2021}, and \emph{index modulation} divides \gls{ris} into reflection elements for passive beamforming and information elements for on-off modulation \cite{Hu2021a}.
	However, those \gls{ris}-empowered \gls{bc} and \gls{sr} designs involve advanced hardware architecture, high optimization complexity, and additional control overhead.



	% Compared to existing \gls{ris}-empowered \gls{sr} that encodes and precodes independently using advanced architecture (e.g., overlay or parallel), Metascatter softly bridges and generalizes parasitic source of \gls{sr} and reflecting element of \gls{ris} via smart input design.

	To the best of our knowledge, all existing \gls{sr} literatures assumed backscatter modulation employs either Gaussian codebook \cite{Guo2019b,Ding2020,Long2020a,Zhou2019a,Wu2021a,Xu2021a,Yang2021a} or finite equiprobable inputs \cite{Yang2018,Han2021,Zhang2022,Xu2020b,Hua2022,Hu2021a}.
	The former is impractical for passive nodes with constrained number of states, while the latter does not fully exploit \gls{csi} to boost achievable backscatter rate.
	Besides, most relevant designs \cite{Guo2019b,Ding2020,Long2020a,Zhou2019a,Wu2021a,Xu2021a,Yang2021a,Yang2018,Han2021,Zhang2022,Xu2020b,Hua2022} were built over ideal \gls{ml} or \gls{sic} receiver.
	However, the advantage of \gls{sic} is questionable because 1) it requires non-coherent primary encoding at the transmitter and re-encoding, precoding, and subtraction at the receiver, 2) the primary and backscatter symbols are mixed by multiplication instead of superposition, and 3) the backscatter symbol period is typically much longer due to physical constraints.
	% Motivated by above, we propose a novel Metascatter network where multiple battery-free tags ride over a conventional point-to-point system and adapt their input probability distribution to simultaneously act as backscatter tags of \gls{sr} and reflecting elements of \gls{ris}.
	Motivated by those, we propose the concept of Metascatter, which adapts the input distribution of a passive backscatter node to generalize backscatter sources of \gls{sr} and reflecting elements of \gls{ris}.
	% An application scenario was considered where multiple Metascatters are introduced to conventional point-to-point system and adapt their input probability distribution to simultaneously act as backscatter tags of \gls{sr} and reflecting elements of \gls{ris}.
	% An application scenario was considered where multiple Metascatters ride over a point-to-point transmission and simultaneously perform backscatter encoding and passive beamforming.
	% ! An application scenario was considered where multiple Metascatters ride over a point-to-point transmission to simultaneously encode self message and perform passive beamforming.
	% To fully accommodate backscatter characteristics, we also propose a novel receiving strategy that first jointly decodes all Metascatter from accumulated energy, then models their reflection patterns and backscatter paths within equivalent channel for primary decoding.
	% minor modifications on existing receivers, easy to perform, fits backscatter characteristics.
	The contributions of this paper is summarized as follows.

	% ! Φ(c[n]) vs P
	\emph{First,} Metascatter adapts the input probability distribution of a finite-state passive backscatter device based on primary and (cascaded) backscatter \gls{csi} to unify and generalize parasitic source of \gls{sr} and reflecting elements of \gls{ris}.
	The reflection pattern over time is no longer fully random or deterministic, but can be flexibly distributed to balance backscatter encoding and passive beamforming.
	For single-user scenario, when primary link is absolutely prioritized, the distribution falls on one state and Metascatter boils down to conventional \gls{ris}.
	When only considering backscatter performance, the distribution involves the highest entropy and Metascatter is essentially an \gls{ambc} node.

	\emph{Second,} we consider an application scenario where multiple Metascatters ride over a point-to-point transmission, exploiting additional propagation paths to simultaneously transmit and assist.
	To fully accommodate backscatter characteristics, we also propose a novel receiving strategy that first jointly decodes all Metascatter from accumulated energy, then models their reflection patterns and backscatter paths within equivalent channel for primary decoding.
	Since backscatter message is modulated over primary signal, backscatter decoding is indeed part of primary channel training, and there is no need for operation-intensive \gls{sic} at the receiver.

	% \emph{Second,} we propose a novel receiving strategy where the backscatter symbol of all tags are first recovered by energy-based decoding, then modeled within equivalent \gls{csi} for primary decoding.
	% When the ratio of backscatter symbol period over primary symbol period is large enough, the randomness of primary source can be averaged out and the performance of non-coherent backscatter decoding can be greatly improved.
	% Since the primary and backscatter symbols are mixed by multiplication coding, the backscatter decoding can be viewed as part of primary channel training, which avoids non-coherent encoding at the transmitter and \gls{sic} at the receiver.

	\emph{Third,} we evaluate the achievable primary-(total-)backscatter rate region by optimizing the input distribution at Metascatters, the active beamforming at the \gls{ap}, and the decision regions at the user. Since the original problem is highly non-convex, we consider a suboptimal \gls{bcd} algorithm where the \gls{kkt} input distribution is numerically evaluated by limit of sequences, the active beamforming is iteratively updated by \gls{pgd} accelerated by backtracking line search, and the decision regions are first restricted to convex, then refined by existing thresholding designs.

	% \emph{Third,} we characterize the achievable primary-(total-)backscatter rate region by iteratively optimizing the tag input distribution, the backscatter decision threshold, and the transmit precoder.
	% For the input design, we propose a numerical method to evaluate the \gls{kkt} solutions by limits of sequences.
	% For the threshold design, we derive the minimum number of decision thresholds to maximize the total backscatter rate, and obtain the optimal thresholds by \gls{dp} accelerated by \gls{smawk} algorithm.
	% However, the optimal precoder design can be non-trivial and we may end up with a low-complexity suboptimal solution.

	\emph{Notations:} Scalars, vectors and matrices are respectively denoted by italic, bold lower-case, and bold upper-case letters.
	$j$ represents the imaginary unit.
	% $\boldsymbol{0}$ and $\boldsymbol{1}$ denote respectively zero and one vector or matrix.
	% $\boldsymbol{I}$ represents the identity matrix.
	% $\mathbb{R}_+^{x \times y}$ and $\mathbb{C}^{x \times y}$ denote respectively the space of real nonnegative and complex $x \times y$ matrices.
	$\mathbb{R}_+^{x \times y}$ and $\mathbb{C}^{x \times y}$ respectively denote the space of real nonnegative and complex $x \times y$ matrices.
	$\Delta^n = \left\{ (p_0,\dots,p_n) \in \mathbb{R}_+^{n+1}|\sum_{i=0}^n t_i = 1 \right\}$ denotes the standard $n$-simplex.
	% $\Re\{\cdot\}$ retrieves the real part of a complex entity. $[\cdot]_{(n)}$ denotes the $n$-th entry of a vector and $[\cdot]_{(1:n)}$ denotes the first $n$ entries of a vector.
	$(\cdot)^*$, $(\cdot)^T$, $(\cdot)^H$, $(\cdot)^+$, $\lvert{\cdot}\rvert$, $\lVert{\cdot}\rVert$ respectively represent the conjugate, transpose, conjugate transpose, ramp function, absolute value, and Euclidean norm.
	% $\arg(\cdot)$, $\mathrm{rank}(\cdot)$, $\mathrm{tr}(\cdot)$, $\mathrm{diag}(\cdot)$ and $\mathrm{diag}^{-1}(\cdot)$ respectively denote the argument, rank, trace, a square matrix with input vector on the main diagonal, and a vector retrieving the main diagonal of the input matrix.
	% $\odot$ denotes the Hadamard product.
	% $\boldsymbol{S} \succeq \boldsymbol{0}$ means $\boldsymbol{S}$ is positive semi-definite.
	% $\mathbb{A}\{\cdot\}$ extracts the DC component of a signal.
	% $\mathbb{E}_X\{\cdot\}$ takes expectation w.r.t. random variable $X$ ($X$ is omitted for simplicity).
	The distribution of a CSCG random vector with mean $\boldsymbol{0}$ and covariance $\boldsymbol{\Sigma}$ is denoted by $\mathcal{CN}(\boldsymbol{0},\boldsymbol{\Sigma})$.
	$\sim$ means ``distributed as''.
	$(\cdot)^{(i)}$ represents the $i$-th iterated value and $(\cdot)^{\star}$ represents the end solution.
\end{section}

\begin{section}{Backscatter Principles}
	% \begin{figure}[!t]
	% 	\centering
	% 	\subfloat[Block Diagram]{
	% 		\resizebox{0.8\columnwidth}{!}{
	% 			\input{assets/illustration/block_diagram.tex}
	% 		}
	% 		\label{fi:block_diagram}
	% 	}
	% 	\\
	% 	\subfloat[Equivalent Circuit]{
	% 		\resizebox{0.95\columnwidth}{!}{
	% 			\input{assets/illustration/equivalent_circuit.tex}
	% 		}
	% 		\label{fi:equivalent_circuit}
	% 	}
	% 	\\
	% 	\subfloat[Scatter Model]{
	% 		\resizebox{0.8\columnwidth}{!}{
	% 			\input{assets/illustration/scatter_model.tex}
	% 		}
	% 		\label{fi:scatter_model}
	% 	}
	% 	\caption{Block diagram, equivalent circuit, and scatter model of a typical passive tag. The solid and dashed vectors represent signal and energy flows. The backscatter antenna behaves as a constant power source, where the voltage $V_0$ and current $I_0$ are introduced by incident electric field $\vec{E}_{\mathrm{I}}$ and magnetic field $\vec{H}_{\mathrm{I}}$ \cite{Huang2021}.}
	% \end{figure}

	\begin{figure*}[!t]
		\centering
		\subfloat[Block Diagram]{
			\resizebox{0.32\linewidth}{!}{
				\input{assets/illustration/block_diagram.tex}
			}
			\label{fi:block_diagram}
		}
		\subfloat[Equivalent Circuit]{
			\resizebox{0.37\linewidth}{!}{
				\input{assets/illustration/equivalent_circuit.tex}
			}
			\label{fi:equivalent_circuit}
		}
		\subfloat[Scatter Model]{
			\resizebox{0.28\linewidth}{!}{
				\input{assets/illustration/scatter_model.tex}
			}
			\label{fi:scatter_model}
		}
		\caption{Block diagram, equivalent circuit, and scatter model of a passive backscatter node. The solid and dashed vectors represent signal and energy flows. The backscatter antenna behaves as a constant power source, where the voltage $V_0$ and current $I_0$ are introduced by incident electric field $\vec{E}_{\mathrm{I}}$ and magnetic field $\vec{H}_{\mathrm{I}}$ \cite{Huang2021}.}
	\end{figure*}

	Passive backscatter nodes harvest energy from and modulate information over surrounding \gls{rf} signals.
	As shown in Fig. \subref*{fi:block_diagram}, a typical passive node consists of a scattering antenna, an energy harvester, an integrated receiver, a load-switching modulator, and on-chip components (e.g., micro-controller and sensors) \cite{Dobkin2012}.
	Its equivalent circuit is presented in Fig. \subref*{fi:equivalent_circuit}.
	% When illuminated, the node absorbs a portion of the impinging wave for information decoding and energy harvesting \cite{Kim2021a}, while scatters the remaining back to the space. The scattered signal is further decomposed into the \emph{structural} component that consistently contributes to environment multipath and covered by channel estimation \cite{Boyer2014}, and the
	When illuminated, the node absorbs a portion of the impinging wave for information decoding and/or energy harvesting \cite{Kim2021a}, and backscatters the remaining as \emph{structural} and \emph{antenna} components.
	% The scattered signal is further decomposed into the \emph{structural} component that consistently contributes to environment multipath and covered by channel estimation \cite{Boyer2014}, and the \emph{antenna} mode component that
	% and \emph{antenna} mode components \cite{Hansen1989}.
	The former consistently contributes to environment multipath and can be modelled by channel estimation \cite{Boyer2014}, while the latter depends on antenna-load impedance mismatch and can be used for backscatter modulation \cite{Boyer2012} and/or channel reconfiguration \cite{Wu2021b}.
	% Depending on structure, shape and material of general objects, structural scattering consistently contributes to environment multipath and is covered by channel estimation \cite{Boyer2014}.
	% In contrast, antenna scattering models radiation pattern from antenna-load impedance mismatch and can be used for backscatter modulation \cite{Boyer2012} and/or channel reconfiguration \cite{Wu2021b}.
	Fig. \subref*{fi:scatter_model} illustrates the scatter model of a node with $M$ states, where the reflection coefficient of state $m \in \mathcal{M} \triangleq \{1,\ldots,M\}$ is defined as%
	\footnote{It corresponds to a linear backscatter model where the reflection coefficient is irrelevant to incident electromagnetic field strength.}
	\begin{equation}
		\Gamma_m = \frac{Z_m - Z_{\mathrm{A}}^*}{Z_m + Z_{\mathrm{A}}},
		\label{eq:reflection_coefficient}
	\end{equation}
	where $Z_m$ is the load impedance at state $m$ and $Z_{\mathrm{A}}$ is the antenna input impedance.

	\begin{subsection}{SR: Backscatter Modulation}
		% Backscatter sources encode self message by \emph{randomly changing reflection states.} For $M$-ary \gls{qam}, reflection coefficient $\Gamma_m$ maps to the corresponding complex constellation point $c_m$ by \cite{Thomas2012a}
		Backscatter sources encode self message by \emph{random reflection states variation.} For $M$-ary \gls{qam}, reflection coefficient $\Gamma_m$ maps to the corresponding \emph{complex constellation point $c_m$} by \cite{Thomas2012a}
		\begin{equation}
			\Gamma_m = \alpha \frac{c_m}{\max_{m'} \lvert c_{m'} \rvert},
			\label{eq:backscatter_modulation}
		\end{equation}
		where $0 \le \alpha \le 1$ is the amplitude reflect ratio that controls the harvest-scatter tradeoff at the direction of interest.
		% When the \gls{csi} and reflection alphabet $\{\Gamma_1,\ldots,\Gamma_M\}$ are available at the reader, it can decode the tag message from the observed scattered signal.
		% ! Use larger alpha in simulation: for high-order QAM, some constellation point may harvests more power than others

	\end{subsection}

	\begin{subsection}{IRS: Channel Reconfiguration}
		% \gls{ris} elements assist legacy transmission by \emph{deterministically choosing phase shifts} based on relevant \gls{csi}. For a reflecting element with $M$ candidate states, reflection coefficient $\Gamma_m$ maps to the corresponding phase shift $\theta_m$ by \cite{Wu2018}
		\gls{ris} elements assist legacy transmission by \emph{deterministic phase shifts selection} based on relevant \gls{csi}.
		For a reflecting element with $M$ candidate states, reflection coefficient $\Gamma_m$ relates to the corresponding \emph{phase shift $\theta_m$} by \cite{Wu2018}
		\begin{equation}
			\Gamma_m = \beta_m \exp(j \theta_m),
		\end{equation}
		where $0 \le \beta_m \le 1$ is overall amplitude reflect ratio of state $m$.
		% Based on relevant \gls{csi}
	\end{subsection}
	% Tags periodically switch between different states to perform backscatter modulation.
	% For $M$-ary \gls{qam}, the complex constellation point $c_m \in \mathcal{C}$ maps to the reflection coefficient by \cite{Thomas2012a}
	% \begin{equation}
	% 	\Gamma_m = \alpha \frac{c_m}{\max_{m'} \lvert c_{m'} \rvert},
	% 	\label{eq:backscatter_modulation}
	% \end{equation}
	% where $\alpha \in [0,1]$ models the harvest-backscatter ratio at the direction of interest.
	% For passive tags, $\alpha \ll 1$ is commonly assumed as the majority of the incident wave can be harvested to support tag modules \cite{Thomas2012a}.

	\begin{subsection}{Metascatter: Bridge and Generalization}
		\begin{figure}[!t]
			\centering
			\subfloat[Input Distribution]{
				\resizebox{0.8\columnwidth}{!}{
					\input{assets/illustration/input_distribution.tex}
				}
			}
			\\
			\subfloat[Time Block]{
				\resizebox{0.8\columnwidth}{!}{
					\input{assets/illustration/time_block.tex}
				}
			}
			\caption{
				Input probability distribution and time block structure of \gls{sr}, \gls{ris}, and Metascatter.
				$T_s$ and $T_c$ respectively denote the primary and backscatter symbol period.
				Within channel coherence time, Metascatter semi-randomly selects reflection state for each backscatter block, with guidance from input probability distribution.
			}
			\label{fi:metascatter}
		\end{figure}
		% For a passive node with finite states, is it possible to unify backscatter modulation and channel reconfiguration by proper selection of reflection coefficient?
		Metascatters simultaneously transmit and assist by \emph{adaptive input distribution design} based on primary and (cascaded) backscatter \gls{csi}.
		% The reflection pattern is neither fully random nor fully deterministic, but flexibly distributed to balance backscatter encoding and passive beamforming.
		% Instead of using fully random or deterministic reflection pattern over time, as shown in Fig. \ref{fi:metascatter}, Metascatter flexibly controls input distribution of candidate reflection states to balance backscatter encoding and passive beamforming.
		% Instead of using fully random or deterministic reflection pattern over time, as shown in Fig. \ref{fi:metascatter}, Metascatter semi-randomly selects reflection state for each backscatter block with guidance from \emph{input probability distribution $P(\Gamma_m)$.}
		% Instead of using fully random or deterministic reflection pattern over time, as shown in Fig. \ref{fi:metascatter}, Metascatter semi-randomly selects reflection state for each backscatter block, with guidance from \emph{input probability distribution $P(\Gamma_m)$.} In other words, it flexibly controls input distribution of candidate reflection states to balance backscatter encoding and passive beamforming. \gls{sr} and \gls{ris} can be regarded as extreme cases of Metascatter, where node input distribution boils down to uniform and deterministically biased, respectively.
		Instead of using fully random or deterministic reflection pattern over time, as shown in Fig. \ref{fi:metascatter}, Metascatter semi-randomly selects reflection state for each backscatter block, with \emph{guidance of input probability $P(\Gamma_m)$} for state $m$. In other words, it flexibly controls input distribution of candidate reflection states to balance backscatter encoding and passive beamforming. \gls{sr} and \gls{ris} can be regarded as extreme cases of Metascatter, where node input distribution boils down to uniform and deterministic, respectively.
		% flexibly controls input distribution of candidate reflection states to balance backscatter encoding and passive beamforming.
		% Metascatter semi-randomly selects reflection state for each backscatter block, with guidance from \emph{input probability distribution $P(\Gamma_m)$.}
		% % Based on \gls{csi}, Metascatter adapts the reflection state distribution to unify data transmission and channel reconfiguration.
		% \gls{sr} and \gls{ris} can be regarded as its extreme cases, where node input distribution boils down to uniform and deterministically biased, respectively.
		% Within channel coherence time, Metascatter semi-randomly selects reflection state for each backscatter block, with guidance from \emph{input probability distribution $P(\Gamma_m)$.}
		\begin{remark}
			Compared to conventional \gls{ris} literatures that optimize phase shifts under unit-module constraint, Metascatter starts from predefined reflection coefficients and designs their input distribution under sum-probability constraint to achieve flexible primary-backscatter tradeoff.
		\end{remark}
		% Next, we will discuss the input design in a Metascatter-enabled network.
		% consider a Metascatter-aided system to evaluate
	\end{subsection}
\end{section}

\begin{section}{Metascatter-Enabled Network}
	\begin{subsection}{System Model}
		\begin{figure}[!t]
			\centering
			\def\svgwidth{0.8\columnwidth}
			\footnotesize{
				\import{assets/illustration/}{metascatter_network.eps_tex}
			}
			\caption{A Metascatter-enabled single-user multi-node network.}
			\label{fi:metascatter_network}
		\end{figure}
		% As shown in Fig. \ref{fi:metascatter_network}, we propose a Metascatter-enabled single-user multi-node network where the spectrum, energy and infrastructures are shared by two coexisting systems.
		As shown in Fig. \ref{fi:metascatter_network}, we propose a Metascatter-enabled single-user multi-node network where two coexisting systems share spectrum, energy and infrastructures.
		% In the primary point-to-point system, the \gls{ap} transmits information to the single-antenna user.
		% In the primary point-to-point system, a $Q$-antenna \gls{ap} transmit to a single-antenna user while assisted by $K$ nearby single-antenna Metascatters.
		The primary point-to-point transmission from a $Q$-antenna \gls{ap} to a single-antenna user is assisted by $K$ nearby single-antenna Metascatters.
		In the secondary backscatter \gls{mac} system, the \gls{ap} serves as the carrier emitter, $K$ nearby single-antenna Metascatters modulate information over reradiated \gls{rf} signals, and the user decodes their messages.
		For simplicity, we consider a quasi-static block fading model where channels remain constant within coherence interval while vary independently between consecutive blocks.
		% Since backscatter modulation involves load switching, tags typically transmit at much lower rate, and we assume the backscatter symbol period is $N \in \mathbb{Z}_{++}$ times the primary symbol period.
		Due to physical constraints on load switching, we assume the backscatter symbol period is $N \gg 1$ times longer than primary and consider integer $N$ without loss of generality.
		We also assume the direct channel and all cascaded channels can be estimated and fed back to the \gls{ap}.%
		\footnote{
			Due to the lack of \gls{rf} chains at the passive tag, accurate and efficient \gls{csi} acquisition at the \gls{ap} can be challenging.
			One possibility is the \gls{ap} sends training pilots, the tags respond in predefined manners, and the user performs least-square estimation with feedbacks \cite{Bharadia2015,Yang2015b,Guo2019g}.
		}
		Besides, we omit the signal reflected by two or more times\cite{Wu2019} and ignore the time difference of arrival from different paths\cite{Guo2019b}.


		Denote the \gls{ap}-user direct channel as $\boldsymbol{h}_{\mathrm{D}}^H \in \mathbb{C}^{1 \times Q}$, the \gls{ap}-node $k \in \mathcal{K} \triangleq \{1,\ldots,K\}$ forward channel as $\boldsymbol{h}_{\mathrm{F},k}^H \in \mathbb{C}^{1 \times Q}$, and the node $k$-user backward channel as $h_{\mathrm{B},k}$. Also, define the cascaded channel of tag $k$ as $\boldsymbol{h}_{\mathrm{C},k}^H \triangleq h_{\mathrm{B},k} \boldsymbol{h}_{\mathrm{F},k}^H \in \mathbb{C}^{1 \times Q}$, and $\boldsymbol{H}_{\mathrm{C}} \triangleq [\boldsymbol{h}_{\mathrm{C},1},\ldots,\boldsymbol{h}_{\mathrm{C},K}]^H \in \mathbb{C}^{K \times Q}$.
		% Let $x_{\mathcal{K}} \triangleq \{x_k : k \in \mathcal{K}\}$ be the backscatter symbols of all Metascatters.
		Let $x_{\mathcal{K}} \triangleq (x_1,\ldots,x_K)$ be the backscatter symbol tuple of all Metascatters.
		Consider the signal model during one backscatter block (i.e., $N$ primary blocks).
		% We assume the primary symbol follows standard \gls{cscg} distribution and the backscatter symbol of all tags employs $M$-\gls{qam}.
		Under perfect synchronization, the equivalent primary channel is a function of \emph{coded} backscatter symbols
		\begin{subequations}
			\label{eq:equivalent_channel}
			\begin{align}
				\boldsymbol{h}_{\mathrm{E}}^H(x_{\mathcal{K}})
				 & \triangleq \boldsymbol{h}_{\mathrm{D}}^H + \sum_{k \in \mathcal{K}} \alpha_k \boldsymbol{h}_{\mathrm{C},k}^H x_k   \\
				 & = \boldsymbol{h}_{\mathrm{D}}^H + \boldsymbol{x}^H \mathrm{diag}(\boldsymbol{\alpha}) \boldsymbol{H}_{\mathrm{C}},
			\end{align}
		\end{subequations}
		where $\alpha_k$ is the amplitude reflect ratio of Metascatter $k$, $\boldsymbol{\alpha} \triangleq [\alpha_1,\ldots,\alpha_K]^T \in \mathbb{R}_+^{K \times 1}$, $x_k \in \mathcal{X} \triangleq \{c_1,\ldots,c_M\}$ is the coded backscatter symbol of Metascatter $k$, and $\boldsymbol{x} \triangleq [x_1,\ldots,x_K]^H \in \mathbb{C}^{K \times 1}$. The signal received by the user at primary block $n \in \mathcal{N} \triangleq \{1,\ldots,N\}$ is
		\begin{equation}
			y[n] = \boldsymbol{h}_{\mathrm{E}}^H(x_{\mathcal{K}}) \boldsymbol{w} s[n] + v[n],
		\end{equation}
		where $s \sim \mathcal{CN}(0,1)$ is the primary symbol, $v \sim \mathcal{CN}(0,\sigma_v^2)$ is the \gls{awgn}, and $\boldsymbol{w} \in \mathbb{C}^{Q \times 1}$ is the active beamforming vector with average power constraint $\lVert \boldsymbol{w} \rVert^2 \le P$.
		% The equivalent channel for primary link is subject to backscatter uncertainty
		% The equivalent primary channel is a function of backscatter symbols  as
		% \begin{equation}
		% 	% \boldsymbol{h}_{\mathrm{E}}^H(x_{\mathcal{K}}) \triangleq \boldsymbol{h}_{\mathrm{D}}^H + \sum_{k \in \mathcal{K}} \sqrt{\alpha_k} \boldsymbol{h}_{\mathrm{C},k}^H x_k.
		% 	\boldsymbol{h}_{\mathrm{E}}^H(\boldsymbol{x}) \triangleq \boldsymbol{h}_{\mathrm{D}}^H + \boldsymbol{x}^H \mathrm{diag}(\boldsymbol{\alpha}) \boldsymbol{H}_{\mathrm{C}}.
		% 	\label{eq:equivalent_channel}
		% \end{equation}

		% where $\alpha_k$ and $x_k$ denote respectively the harvest-backscatter efficiency and backscatter symbol of tag $k$, $s[n]$ and $w[n] \sim \mathcal{CN}(0,\sigma_v^2)$ denote respectively the primary symbol and \gls{awgn} at block $n$, and $\boldsymbol{w} \in \mathbb{C}^{Q \times 1}$ is the transmit precoder satisfying power constraint $\lVert \boldsymbol{w} \rVert^2 \le P$.
		% For the ease of notation, we define $x_{\mathcal{K}} \triangleq \{x_k : k \in \mathcal{K}\}$ as the tag input combination, and $\boldsymbol{y} \triangleq \left[y[1],\ldots,y[N]\right]^T \in \mathbb{C}^{N \times 1}$ as the received signal per backscatter symbol period.
		% For primary transmission, the equivalent channel is subject to backscatter modulation uncertainty as
		% \begin{equation}
		% 	\boldsymbol{h}_{\mathrm{E}}^H(x_{\mathcal{K}}) \triangleq \boldsymbol{h}_{\mathrm{D}}^H + \sum_{k \in \mathcal{K}} \sqrt{\alpha_k} \boldsymbol{h}_{\mathrm{C},k}^H x_k.
		% 	\label{eq:equivalent_channel}
		% \end{equation}

		\begin{remark}
			Metascatter involves a symbiotic \gls{mac} where the primary and backscatter symbols of different duration are mixed by \gls{mc} instead of \gls{sc}.
			For each node, the reflection coefficient not only encodes the backscatter message, but also influences the equivalent primary channel \eqref{eq:equivalent_channel}.
			% As such, novel receiving strategy other than \gls{sic} should be tailored to signal characteristics to unveil how reflection pattern potentially influences the primary-backscatter tradeoff.
			To accommodate such signal characteristics, novel receiving strategy apart from \gls{sic} is desired to better utilize the reflection pattern and boost the primary-backscatter tradeoff.
		\end{remark}
	\end{subsection}

	\begin{subsection}{Receiving Strategy}
		We propose a Metascatter receiver where the backscatter messages of all Metascatters are first jointly and semi-coherently detected using total received energy per backscatter block, then modeled within equivalent channel \eqref{eq:equivalent_channel} as dynamic passive beamforming.
		% identified by non-coherent energy detection, then
		% Compared with conventional schemes as joint decoding and \gls{sic}, the Metascatter receiver may not achieve as high data rate for one tag, but it avoids non-coherent primary encoding and enables tag multiple access in a practical and low-complexity manner.
		% Therefore, we believe it can be compatible to and readily implemented over legacy point-to-point systems.
		Compared with \gls{ml} and \gls{sic}, Metascatter receiver allows practical and low-complexity node multiple access with minor adjustment over legacy equipments.

		% At a specific backscatter block, denote $m_k \in \mathcal{M}$ as the state index of Metascatter $k$, and let $m_{\mathcal{K}} \triangleq \{m_k: k \in \mathcal{K}\}$ collect the state indexes of all Metascatters.
		At a specific backscatter block, denote $m_k \in \mathcal{M}$ as the state index of Metascatter $k$, and let $m_{\mathcal{K}} \triangleq (m_1,\ldots,m_K)$ be the state index tuple of all Metascatters.
		% The received signal at primary block $n$ is subject to the variation of $s[n]$ and $v[n]$, and thus distributed as $y[n] \sim \mathcal{CN}(0,\sigma_v^2)$
		Conditioned on $m_{\mathcal{K}}$, the received signal at primary block $n$ is subject to the variation of $s[n]$ and $v[n]$, distributed as $y[n] \sim \mathcal{CN}(0,\sigma_{m_{\mathcal{K}}}^2)$ with
		\begin{equation}
			% \sigma_{m_{\mathcal{K}}}^2 = \Bigl\lvert \underbrace{\bigl(\boldsymbol{h}_{\mathrm{D}}^H + \sum_{k \in \mathcal{K}} \sqrt{\alpha_k} \boldsymbol{h}_{\mathrm{C},k}^H x_{m_k}\bigr)}_{\boldsymbol{h}_{\mathrm{E}}^H(x_{m_{\mathcal{K}}})} \boldsymbol{w} \Bigr\rvert^2 + \sigma_v^2,
			\sigma_{m_{\mathcal{K}}}^2 = \lvert \boldsymbol{h}_{\mathrm{E}}^H(x_{m_{\mathcal{K}}}) \boldsymbol{w} \rvert^2 + \sigma_v^2,
			\label{eq:receive_variance}
		\end{equation}
		where $x_{m_k}$ and $x_{m_\mathcal{K}}$ are the symbol and symbol tuple associated with state $m_k$ and state tuple $m_{\mathcal{K}}$, respectively.%
		\footnote{
			$x_k$ and $x_{\mathcal{K}}$ are random variables, while $x_{m_k}$ and $x_{m_{\mathcal{K}}}$ are their instances indexed by $m_k$ and $m_{\mathcal{K}}$.
		}
		Also, denote the total received energy within backscatter block as $z=\sum_{n=1}^N \lvert y[n] \rvert^2$.
		% As the sum of $N$ \gls{iid} exponential variables, conditioned on $m_{\mathcal{K}}$, its \gls{pdf} follows Erlang distribution
		As the sum of $N$ \gls{iid} exponential variables, the \gls{pdf} of $z$ conditioned on $m_{\mathcal{K}}$ follows Erlang distribution
		\begin{equation}
			f(z|\mathcal{H}_{m_{\mathcal{K}}}) = \frac{z^{N-1} \exp(-z/\sigma_{m_{\mathcal{K}}}^2)}{\sigma_{m_{\mathcal{K}}}^{2N} (N-1)!},
			% f(z|\mathcal{H}_{m_{\mathcal{K}}}) = \frac{z^{N-1} \exp(\frac{-z}{\sigma_{m_{\mathcal{K}}}^2})}{\sigma_{m_{\mathcal{K}}}^{2N} (N-1)!},
			\label{eq:energy_distribution}
		\end{equation}
		where $\mathcal{H}_{m_{\mathcal{K}}}$ denotes hypothesis $m_{\mathcal{K}}$.
		% To accommodate backscatter characteristics and reduce decoding complexity, we consider a joint semi-coherent energy detection for all Metascatters based on disjoint decision regions over accumulated energy $z$.
		To accommodate backscatter characteristics and reduce decoding complexity, we consider a joint semi-coherent detection for all Metascatters over accumulated energy $z$.
		% To reduce decoding complexity, we consider a joint semi-coherent energy detection for all Metascatters based on disjoint decision regions over accumulated energy $z$.
		\begin{figure}[!t]
			\centering
			\resizebox{0.9\columnwidth}{!}{
				\input{assets/illustration/energy_distribution.tex}
			}
			% \caption{\gls{pdf} of Average Received Power Conditioned on Different Input Hypothesis. Example \gls{ml}.}
			\caption{
				\gls{pdf} of total received energy per backscatter block, conditioned on different input hypothesis.
				% This \gls{ml} decision consists of convex regions and is generally rate-suboptimal except for equiprobable inputs.}
				Here, the convex \gls{ml} decision regions are generally rate-suboptimal except for equiprobable inputs.
			}
			\label{fi:energy_distribution}
		\end{figure}
		% The idea is presented in Fig. \ref{fi:energy_distribution}.
		% As illustrated in Fig. \ref{fi:energy_distribution}, o
		Once disjoint energy decision regions are determined, we can construct a \gls{dtmac} and formulate the transition probability from input $x_{m_{\mathcal{K}}}$ to output $\hat{x}_{m_{\mathcal{K}}'}$ as
		% it essentially formulates a \gls{dtmac}, and the transition probability from input $x_{m_{\mathcal{K}}}$ to output $\hat{x}_{m_{\mathcal{K}}'}$ is
		\begin{equation}
			P(\hat{x}_{m_{\mathcal{K}}'}|x_{m_{\mathcal{K}}}) = \int_{\mathcal{R}_{m_{\mathcal{K}}'}} f(z|\mathcal{H}_{m_{\mathcal{K}}}) \, d z,
			\label{eq:dtmac}
		\end{equation}
		where $\mathcal{R}_{m_{\mathcal{K}}'}$ is the decision region of hypothesis $\mathcal{H}_{m_{\mathcal{K}}'}$. An example of \gls{ml} energy decision is illustrated in Fig. \ref{fi:energy_distribution}.

		\begin{remark}
			% For a general input distribution, not only the optimal decision thresholds
			The rate-optimal thresholding channel design remains under-investigated, and some attempts were made for single source with binary inputs in \cite{Qian2019b,Nguyen2021b}.
			% The question was only answered for single source with binary inputs in \cite{Qian2019b,Nguyen2021b}.
			% Some attempts for single source with binary inputs were presented in \cite{Qian2019b,Nguyen2021b}.
			For non-binary inputs with general distribution, the optimal decision region for each letter can be non-convex (i.e., with non-adjacent partitions) and the optimal number of thresholds is still unknown.
			% Next, we will restrict the design over convex decision regions.
			% For a general input distribution, not only the rate-optimal decision thresholds
			% % Interestingly, the optimal threshold design to maximize the mutual information for a general \gls{dmtc} with a fixed number of output letters remains an open issue.
			% % The reason is that each decision region may contain more than one disjoint partitions (i.e., non-convex) and the number of thresholds are unknown.
			% % Fortunately, for the proposed energy detection, we proved that the \gls{dmtc} capacity can be achieved using only convex decision regions.
			% not only the capacity-achieving thresholding design remains an open problem, but also the optimal number of thresholds for general non-binary-input channels remains unknown.
		\end{remark}
		In the following context, we restrict all decision regions to convex and optimize decision thresholds accordingly. That is, for any bijective mapping $f : m_{\mathcal{K}} \to \mathcal{L} \triangleq \{1,\ldots,M^K\}$, the decision region of letter $l \in \mathcal{L}$ is defined as $\mathcal{R}_l \triangleq [t_{l-1},t_l)$, where $t_{l-1} \le t_l$. We also define the decision threshold vector as $\boldsymbol{t} \triangleq [t_0,\ldots,t_L]^T \in \mathbb{R}_+^{(L+1) \times 1}$.
	\end{subsection}

	\begin{subsection}{Achievable Rates}
		Denote the input probability of state $m_k$ of Metascatter node $k$ as $P_k(x_{m_k})$, and define the input probability distribution vector of node $k$ as $\boldsymbol{p}_k \triangleq [P_k(c_1),\ldots,P_k(c_M)]^T \in \mathbb{R}^{M \times 1}$. With independent encoding at all nodes, the probability of backscatter symbol tuple $x_{m_{\mathcal{K}}}$ is
		\begin{equation}
			P_{\mathcal{K}}(x_{m_{\mathcal{K}}}) = \prod_{k \in \mathcal{K}} P_k(x_{m_k}).
			\label{eq:equivalent_distribution}
		\end{equation}
		% Denote the input probability distribution vector of tag $k$ as $\boldsymbol{p}_k \triangleq [P_k(x_1),\ldots,P_k(x_M)]^T \in \mathbb{R}^{M \times 1}$, where $P_k(x_{m_k})$ is the probability at state $m_k$.
		% Consider independent encoding at all tags such that the probability of input combination $x_{m_{\mathcal{K}}}$ is $P_{\mathcal{K}}(x_{m_{\mathcal{K}}}) = \prod_{k \in \mathcal{K}} P_k(x_{m_k})$.
		% The backscatter information function of input combination $x_{m_{\mathcal{K}}}$ is defined as
		Similar to \cite{Rezaeian2004}, we define the backscatter information function between input symbol tuple instance $x_{m_{\mathcal{K}}}$ and output symbol tuple $\hat{x}_{\mathcal{K}}$ as
		\begin{equation}
			I^{\mathrm{B}}(x_{m_{\mathcal{K}}};\hat{x}_{\mathcal{K}}) \triangleq \sum_{m_{\mathcal{K}}'} P(\hat{x}_{m_{\mathcal{K}}'}|x_{m_{\mathcal{K}}}) \log \frac{P(\hat{x}_{m_{\mathcal{K}}'}|x_{m_{\mathcal{K}}})}{P(\hat{x}_{m_{\mathcal{K}}'})},
			\label{eq:backscatter_information_function}
		\end{equation}
		where $P(\hat{x}_{m_{\mathcal{K}}'}) = \sum_{m_{\mathcal{K}}} P_{\mathcal{K}}(x_{m_{\mathcal{K}}}) P(\hat{x}_{m_{\mathcal{K}}'}|x_{m_{\mathcal{K}}})$.
		We also define the backscatter marginal information of letter $x_{m_k}$ of node $k$ as
		% Besides, the backscatter marginal information function associated with letter $x_{m_k}$ of tag $k$ is
		\begin{equation}
			% I_{\mathrm{B},k}(x_{m_k};\hat{x}_{\mathcal{K}}) \triangleq \sum_{m_{\mathcal{K} \setminus \{k\}}} P_{\mathcal{K} \setminus \{k\}}(x_{m_{\mathcal{K} \setminus \{k\}}}) I_{\mathrm{B}}(x_{m_{\mathcal{K}}};\hat{x}_{\mathcal{K}}),
			I^{\mathrm{B}}_{k}(x_{m_k};\hat{x}_{\mathcal{K}}) \triangleq \sum_{m_{\mathcal{K} \setminus \{k\}}} P_{\mathcal{K} \setminus \{k\}}(x_{m_{\mathcal{K} \setminus \{k\}}}) I^{\mathrm{B}}(x_{m_{\mathcal{K}}};\hat{x}_{\mathcal{K}}),
			\label{eq:backscatter_marginal_information}
		\end{equation}
		where $P_{\mathcal{K} \setminus \{k\}}(x_{m_{\mathcal{K} \setminus \{k\}}}) = \prod_{q \in \mathcal{K} \setminus \{k\}} P_{q}(x_{m_{q}})$.
		Moreover, we can write the backscatter mutual information as
		\begin{equation}
			% I^{\mathrm{B}}(x_{\mathcal{K}};\hat{x}_{\mathcal{K}}) = \sum_{m_{\mathcal{K}}} P_{\mathcal{K}}(x_{m_{\mathcal{K}}}) \sum_{m_{\mathcal{K}}'} P(\hat{x}_{m_{\mathcal{K}}'}|x_{m_{\mathcal{K}}}) \log \frac{P(\hat{x}_{m_{\mathcal{K}}'}|x_{m_{\mathcal{K}}})}{P(\hat{x}_{m_{\mathcal{K}}'})}.
			I^{\mathrm{B}}(x_{\mathcal{K}};\hat{x}_{\mathcal{K}}) = \sum_{m_{\mathcal{K}}} P_{\mathcal{K}}(x_{m_{\mathcal{K}}}) I^{\mathrm{B}}(x_{m_{\mathcal{K}}};\hat{x}_{\mathcal{K}}).
			\label{eq:backscatter_mutual_information}
		\end{equation}

		% Once the tag input combination is successfully decoded, the backscatter uncertainty can be eliminated and the equivalent channel for primary transmission can be updated by \eqref{eq:equivalent_channel}.

		% eliminate modulation uncertainty
		Once backscatter messages are successfully decoded, we can re-encode to determine $x_{\mathcal{K}}$ and retrieve equivalent primary channel by \eqref{eq:equivalent_channel}. We define the primary information function conditioned on backscatter symbol tuple $x_{m_{\mathcal{K}}}$ as
		% Once backscatter symbols are successfully decoded, we can eliminate modulation uncertainty and retrieve equivalent primary channel by \eqref{eq:equivalent_channel}. We define the primary information function conditioned on backscatter symbol tuple $x_{m_{\mathcal{K}}}$ as
		% \begin{equation}
		% 	I_{\mathrm{P}}(x_{m_{\mathcal{K}}};\boldsymbol{y}) \triangleq \log \left(1 + \frac{\lvert \boldsymbol{h}_{\mathrm{E}}^H(x_{m_{\mathcal{K}}}) \boldsymbol{w} \rvert^2}{\sigma_v^2}\right),
		% 	\label{eq:primary_information_function}
		% \end{equation}
		\begin{equation}
			I^{\mathrm{P}}(s;y|x_{m_{\mathcal{K}}}) \triangleq \log \Bigl(1 + \frac{\lvert \boldsymbol{h}_{\mathrm{E}}^H(x_{m_{\mathcal{K}}}) \boldsymbol{w} \rvert^2}{\sigma_v^2}\Bigr),
			\label{eq:primary_information_function}
		\end{equation}
		the primary marginal information conditioned on letter $x_{m_k}$ of node $k$ as
		% the primary marginal information function associated with letter $x_{m_k}$ of tag $k$ is
		\begin{equation}
			I^{\mathrm{P}}_{k}(s;y|x_{m_k}) \triangleq \sum_{m_{\mathcal{K} \setminus \{k\}}} P_{\mathcal{K} \setminus \{k\}}(x_{m_{\mathcal{K} \setminus \{k\}}}) I^{\mathrm{P}}(s;y|x_{m_{\mathcal{K}}}),
			\label{eq:primary_marginal_information}
		\end{equation}
		and the primary ergodic mutual information as
		% and the primary (ergodic) mutual information is
		\begin{equation}
			% I^{\mathrm{P}}(s;y|x_{\mathcal{K}}) = \sum_{m_{\mathcal{K}}} P_{\mathcal{K}}(x_{m_{\mathcal{K}}}) \log \left(1 + \frac{\lvert \boldsymbol{h}_{\mathrm{E}}^H(x_{m_{\mathcal{K}}}) \boldsymbol{w} \rvert^2}{\sigma_v^2}\right).
			I^{\mathrm{P}}(s;y|x_{\mathcal{K}}) = \sum_{m_{\mathcal{K}}} P_{\mathcal{K}}(x_{m_{\mathcal{K}}}) I^{\mathrm{P}}(s;y|x_{m_{\mathcal{K}}}).
			\label{eq:primary_mutual_information}
		\end{equation}

		% Moreover, with $\rho \in [0,1]$ being the relative priority of the primary link, we define corresponding weighted sum information function, marginal information, and mutual information respectively as
		Finally, with a slight abuse of notation, we define the corresponding weighted sum information function, marginal information, and mutual information respectively as
		\begin{align}
			I(x_{m_{\mathcal{K}}})
			 & \triangleq \rho I^{\mathrm{P}}(s;y|x_{m_{\mathcal{K}}}) + (1 - \rho) I^{\mathrm{B}}(x_{m_{\mathcal{K}}};\hat{x}_{\mathcal{K}}),\label{eq:weighted_sum_information_function} \\
			I_k(x_{m_k})
			 & \triangleq \rho I^{\mathrm{P}}_{k}(s;y|x_{m_k}) + (1 - \rho) I^{\mathrm{B}}_{k}(x_{m_k};\hat{x}_{\mathcal{K}}),\label{eq:weighted_sum_marginal_information}                 \\
			I(x_{\mathcal{K}})
			 & \triangleq \rho I^{\mathrm{P}}(s;y|x_{\mathcal{K}}) + (1 - \rho) I^{\mathrm{B}}(x_{\mathcal{K}};\hat{x}_{\mathcal{K}}),\label{eq:weighted_sum_mutual_information}
		\end{align}
		where $0 \le \rho \le 1$ is the relative priority of the primary link.


		% Therefore, the weighted sum information function, marginal information function, and mutual information of primary and backscatter links are respectively given by
		% \begin{align}
		% 	I(x_{m_{\mathcal{K}}};\hat{x}_{\mathcal{K}},\boldsymbol{y})
		% 	 & \triangleq \rho I_{\mathrm{P}}(x_{m_{\mathcal{K}}};\boldsymbol{y}) + (1 - \rho) I_{\mathrm{B}}(x_{m_{\mathcal{K}}};\hat{x}_{\mathcal{K}}),\label{eq:weighted_sum_information_function} \\
		% 	I_k(x_{m_k};\hat{x}_{\mathcal{K}},\boldsymbol{y})
		% 	 & \triangleq \rho I_{\mathrm{P},k}(x_{m_k};\boldsymbol{y}) + (1 - \rho) I_{\mathrm{B},k}(x_{m_k};\hat{x}_{\mathcal{K}}),\label{eq:weighted_sum_marginal_information}                     \\
		% 	I(x_{\mathcal{K}};\hat{x}_{\mathcal{K}},\boldsymbol{y})
		% 	 & \triangleq \rho I_{\mathrm{P}}(x_{\mathcal{K}};\boldsymbol{y}) + (1 - \rho) I_{\mathrm{B}}(x_{\mathcal{K}};\hat{x}_{\mathcal{K}}),\label{eq:weighted_sum_mutual_information}
		% \end{align}
		% where $\rho \in [0,1]$ represents the priority of the primary link.
	\end{subsection}
\end{section}

\begin{section}{Input Distribution, Active Beamforming, and Decision Threshold Design}
	To characterize the achievable primary-(total-)backscatter rate region of the proposed Metascatter-enabled network, we aim to maximize the weighted sum mutual information with respect to node input distributions $\{\boldsymbol{p}_k\}_{k \in \mathcal{K}}$, active beamforming vector $\boldsymbol{w}$, and decision threshold vector $\boldsymbol{t}$ as
	\begin{maxi!}
		{\scriptstyle{\{\boldsymbol{p}_k\}_{k \in \mathcal{K}},\boldsymbol{w},\boldsymbol{t}\in\mathbb{R}_+^{L+1}}}{I(x_{\mathcal{K}})}{\label{op:weighted_sum_rate}}{\label{ob:weighted_sum_rate}}
		% \addConstraint{\sum \nolimits_{m_k} P_k(x_{m_k})}{=1,}{\quad \forall k \in \mathcal{K}}{\label{co:sum_probability}}
		\addConstraint{\sum \nolimits_{m_k} P_k(x_{m_k})}{=1,}{\quad \forall k}{\label{co:sum_probability}}
		% \addConstraint{P_k(x_{m_k})}{\ge 0,}{\quad \forall k \in \mathcal{K}, \ \forall m_k \in \mathcal{M}}{\label{co:nonnegative_probability}}
		\addConstraint{P_k(x_{m_k})}{\ge 0,}{\quad \forall k,m_k}{\label{co:nonnegative_probability}}
		\addConstraint{\lVert \boldsymbol{w} \rVert^2}{\le P}{\label{co:transmit_power}}
		\addConstraint{t_{l-1}}{\le t_l,}{\quad \forall l.}{\label{co:decision_threshold}}
	\end{maxi!}
	% \begin{maxi!}
	% 	{\scriptstyle{\{\boldsymbol{p}_k\}_{k \in \mathcal{K}},\boldsymbol{t},\boldsymbol{w}}}{I(x_{\mathcal{K}})}{\label{op:weighted_sum_rate}}{\label{ob:weighted_sum_rate}}
	% 	\addConstraint{\boldsymbol{p}_k}{\in \Delta^{M-1},}{\quad \forall k \in \mathcal{K}}{\label{co:sum_probability}}
	% 	% \addConstraint{P_k(x_{m_k})}{\ge 0,}{\quad \forall k \in \mathcal{K}, \ \forall m_k \in \mathcal{M}}{\label{co:nonnegative_probability}}
	% 	\addConstraint{\lVert \boldsymbol{w} \rVert^2}{\le P.}{\label{co:transmit_power}}
	% \end{maxi!}
	% \begin{maxi!}
	% 	{\scriptstyle{\boldsymbol{p}_k \in \Delta^{M-1},\boldsymbol{t},\boldsymbol{w}}}{I(x_{\mathcal{K}})}{\label{op:weighted_sum_rate}}{\label{ob:weighted_sum_rate}}
	% 	% \addConstraint{\boldsymbol{p}_k}{\in \Delta^{M-1},}{\quad \forall k \in \mathcal{K}}{\label{co:sum_probability}}
	% 	% \addConstraint{P_k(x_{m_k})}{\ge 0,}{\quad \forall k \in \mathcal{K}, \ \forall m_k \in \mathcal{M}}{\label{co:nonnegative_probability}}
	% 	\addConstraint{\lVert \boldsymbol{w} \rVert^2}{\le P.}{\label{co:transmit_power}}
	% \end{maxi!}
	Problem \eqref{op:weighted_sum_rate} is highly non-convex, and we propose a \gls{bcd} algorithm that iteratively updates $\{\boldsymbol{p}_k\}_{k \in \mathcal{K}}$, $\boldsymbol{w}$ and $\boldsymbol{t}$ until convergence.

	\begin{subsection}{Input Distribution}
		% For any fixed decision boundary $\boldsymbol{t}$ and transmit precoder $\boldsymbol{w}$, the equivalent \gls{dtmac} can be formulated by \eqref{eq:dtmac} and problem \eqref{op:weighted_sum_rate} boils down to
		For any given $\boldsymbol{w}$ and $\boldsymbol{t}$, we can construct the equivalent \gls{dtmac} by \eqref{eq:dtmac} and simplify \eqref{op:weighted_sum_rate} to
		\begin{maxi!}
			{\scriptstyle{\{\boldsymbol{p}_k\}_{k \in \mathcal{K}}}}{I(x_{\mathcal{K}})}{\label{op:input_distribution}}{}
			\addConstraint{\eqref{co:sum_probability},\eqref{co:nonnegative_probability},}
		\end{maxi!}
		which involves coupled term $\prod_{k \in \mathcal{K}} P_k(x_{m_k})$ and is non-convex when $K > 1$. Next, we propose a numerical method that evaluate the \gls{kkt} input distribution by limit of sequences.
		\begin{remark}
			% As pointed out in \cite{Buhler2011}, \gls{kkt} conditions are only necessary for such kind of problems and these solutions may end up being saddle points.
			As pointed out in \cite{Buhler2011}, \gls{kkt} conditions are generally necessary but insufficient for total rate maximization in discrete memoryless \gls{mac}. Therefore, \gls{kkt} solutions may end up being saddle points of problem \eqref{op:input_distribution}.
		\end{remark}
		Following \cite{Rezaeian2004}, we first recast \gls{kkt} conditions to their equivalent form for problem \eqref{op:input_distribution}, then propose an iterative method that guarantees input distribution satisfying above conditions on convergence.
		% Interestingly, the total-rate optimal input design for general discrete memoryless \gls{mac} remains an open problem, and we instead propose a \gls{kkt} solution to problem \eqref{op:input_distribution}.
		\begin{proposition}
			% The \gls{kkt} optimality conditions for problem \eqref{op:input_distribution} are equivalent to, $\forall k \in \mathcal{K}$ and $\forall m_k \in \mathcal{M}$,
			The \gls{kkt} optimality conditions for problem \eqref{op:input_distribution} are equivalent to, $\forall k,m_k$,
			\begin{subequations}
				\label{eq:input_kkt_condition}
				\begin{alignat}{2}
					I_k^\star(x_{m_k}) & = I^\star(x_{\mathcal{K}}), \quad   &  & P_k^\star(x_{m_k}) > 0,\label{eq:probable_states} \\
					I_k^\star(x_{m_k}) & \le I^\star(x_{\mathcal{K}}), \quad &  & P_k^\star(x_{m_k}) = 0.\label{eq:dropped_states}
				\end{alignat}
			\end{subequations}
			\label{pr:input_kkt_condition}
		\end{proposition}

		\begin{proof}
			Please refer to Appendix \ref{ap:input_kkt_condition}.
			\label{pf:input_kkt_condition}
		\end{proof}

		For each node, \eqref{eq:probable_states} suggests each probable state should produce the same marginal information (averaged over all states of other nodes), while \eqref{eq:dropped_states} implies any state with potentially less marginal information should not be used.
		% We notice \eqref{eq:probable_states} means each probable state of each tag should produce the same marginal information (averaged over all states of other tags), while \eqref{eq:dropped_states} implies any state of any tag with potentially less marginal information than above should not be used.
		% Next, we generalize the Blahut-Arimoto algorithm \cite{Arimoto1972,Blahut1972a} and propose a numerical evaluation of \gls{kkt} points by limits of sequences.
		\begin{proposition}
			% The \gls{kkt} solution of input probability of tag $k$ at state $m_k$ is given by the converging point of the sequence
			The \gls{kkt} input probability of node $k$ of state $m_k$ is given by the converging point of the sequence
			% The \gls{kkt} solution of input probability of tag $k$ at state $m_k$ is given by the converging point of the sequence
			\begin{equation}
				P_k^{(r+1)}(x_{m_k}) = \frac{P_k^{(r)}(x_{m_k}) \exp \Bigl( \frac{\rho}{1 - \rho} I_k^{(r)}(x_{m_k}) \Bigr)}{\sum_{m_k'} P_k^{(r)}(x_{m_k'}) \exp \Bigl( \frac{\rho}{1 - \rho} I_k^{(r)}(x_{m_k'}) \Bigr)},
				\label{eq:input_kkt_solution}
			\end{equation}
			% where $r$ is the iteration index and $\boldsymbol{p}_k^{(0)} > \boldsymbol{0}$, $\forall k \in \mathcal{K}$.
			where $r$ is the iteration index and $\boldsymbol{p}_k^{(0)} > \boldsymbol{0}$, $\forall k$.
			\label{pr:input_kkt_solution}
		\end{proposition}
		\begin{proof}
			Please refer to Appendix \ref{ap:input_kkt_solution}.
			\label{pf:input_kkt_solution}
		\end{proof}

		At each iteration, the input distribution of node $k$ is evaluated over updated input distribution of node $1$ to $k-1$, together with previous input distribution of node $k+1$ to $K$. The \gls{kkt} input distribution design is summarized in Algorithm \ref{al:input_distribution}.

		\begin{algorithm}[!t]
			\caption{Numerical Evaluation of \gls{kkt} Input Distribution}
			\label{al:input_distribution}
			\begin{algorithmic}[1]
				\Require $K$, $N$, $\boldsymbol{h}_{\mathrm{D}}^H$, $\boldsymbol{H}_{\mathrm{C}}$, $\boldsymbol{\alpha}$, $\mathcal{X}$, $\sigma_v^2$, $\rho$, $\boldsymbol{w}$, $\boldsymbol{t}$, $\epsilon$
				\Ensure $\{\boldsymbol{p}_k^\star\}_{k \in \mathcal{K}}$
				\State Set $\boldsymbol{h}_{\mathrm{E}}^H(x_{m_{\mathcal{K}}})$, $\forall m_{\mathcal{K}}$ by \eqref{eq:equivalent_channel}
				\State \phantom{Set} $\sigma^2_{m_{\mathcal{K}}}$, $\forall m_{\mathcal{K}}$ by \eqref{eq:receive_variance}
				\State \phantom{Set} $f(z|\mathcal{H}_{m_{\mathcal{K}}})$, $\forall m_{\mathcal{K}}$ by \eqref{eq:energy_distribution}
				\State \phantom{Set} $P(\hat{x}_{m_{\mathcal{K}}'}|x_{m_{\mathcal{K}}})$, $\forall m_{\mathcal{K}}, m_{\mathcal{K}}'$ by \eqref{eq:dtmac}
				\State Initialize $r \gets 0$
				\State \phantom{Initialize} $\boldsymbol{p}_k^{(0)} > \boldsymbol{0}$, $\forall k$
				\State Get $P_{\mathcal{K}}^{(r)}(x_{m_{\mathcal{K}}})$, $\forall m_{\mathcal{K}}$ by \eqref{eq:equivalent_distribution} \label{st:input_distribution_begin}
				\State \phantom{Get} $I^{(r)}(x_{m_{\mathcal{K}}})$, $\forall m_{\mathcal{K}}$ by \eqref{eq:backscatter_information_function}, \eqref{eq:primary_information_function}, \eqref{eq:weighted_sum_information_function}
				\State \phantom{Get} $I^{(r)}_k(x_{m_k})$, $\forall k,m_k$ by \eqref{eq:backscatter_marginal_information}, \eqref{eq:primary_marginal_information}, \eqref{eq:weighted_sum_marginal_information}
				\State \phantom{Get} $I^{(r)}(x_{\mathcal{K}})$ by \eqref{eq:backscatter_mutual_information}, \eqref{eq:primary_mutual_information}, \eqref{eq:weighted_sum_mutual_information} \label{st:input_distribution_end}
				\Repeat
					\State Update $r \gets r+1$
					\State \phantom{Update} $\boldsymbol{p}_k^{(r)}$, $\forall k$ by \eqref{eq:input_kkt_solution}
					\State Redo step \ref{st:input_distribution_begin}--\ref{st:input_distribution_end}
					% \State Get $P_{\mathcal{K}}^{(r)}(x_{m_{\mathcal{K}}})$, $\forall m_{\mathcal{K}}$ by \eqref{eq:equivalent_distribution}
					% \State \phantom{Get} $I^{(r)}(x_{m_{\mathcal{K}}})$, $\forall m_{\mathcal{K}}$ by \eqref{eq:backscatter_information_function}, \eqref{eq:primary_information_function}, \eqref{eq:weighted_sum_information_function}
					% \State \phantom{Get} $I^{(r)}_k(x_{m_k})$, $\forall k,m_k$ by \eqref{eq:backscatter_marginal_information}, \eqref{eq:primary_marginal_information}, \eqref{eq:weighted_sum_marginal_information}
					% \State \phantom{Get} $I^{(r)}(x_{\mathcal{K}})$ by \eqref{eq:backscatter_mutual_information}, \eqref{eq:primary_mutual_information}, \eqref{eq:weighted_sum_mutual_information}
				\Until $I^{(r)}(x_{\mathcal{K}}) - I^{(r-1)}(x_{\mathcal{K}}) \le \epsilon$
			\end{algorithmic}
		\end{algorithm}
	\end{subsection}

	\begin{subsection}{Active Beamforming}
		For any given $\{\boldsymbol{p}_k\}_{k \in \mathcal{K}}$ and $\boldsymbol{t}$, problem \eqref{op:weighted_sum_rate} reduces to
		\begin{maxi!}
			{\scriptstyle{\boldsymbol{w}}}{I(x_{\mathcal{K}})}{\label{op:active_beamforming}}{\label{ob:active_beamforming}}
			\addConstraint{\eqref{co:transmit_power},}
		\end{maxi!}
		which is still non-convex due to integration and entropy terms.
		\begin{figure*}[!b]
			\hrule
			\begin{equation}
				I(x_{\mathcal{K}})=\sum_{m_{\mathcal{K}}}P_{\mathcal{K}}(x_{m_{\mathcal{K}}})\Biggl(\rho\log\Bigl(1+\frac{\lvert\boldsymbol{h}_{\mathrm{E}}^H(x_{m_{\mathcal{K}}})\boldsymbol{w}\rvert^2}{\sigma_v^2}\Bigr)+(1-\rho)\sum_l Q\Bigl(N,\frac{t_{l-1}}{\sigma_{m_{\mathcal{K}}}^2},\frac{t_l}{\sigma_{m_{\mathcal{K}}}^2}\Bigr) \log \frac{Q\Bigl(N,\frac{t_{l-1}}{\sigma_{m_{\mathcal{K}}}^2},\frac{t_l}{\sigma_{m_{\mathcal{K}}}^2}\Bigr)}{\sum_{m_{\mathcal{K}}'} P_{\mathcal{K}}(x_{m_{\mathcal{K}}'}) Q\Bigl(N,\frac{t_{l-1}}{\sigma_{m_{\mathcal{K}}'}^2},\frac{t_l}{\sigma_{m_{\mathcal{K}}'}^2}\Bigr)}\Biggr)
				\label{eq:weighted_sum_mutual_information_explicit}
			\end{equation}
		\end{figure*}
		To see this, we explicitly write \eqref{ob:active_beamforming} as \eqref{eq:weighted_sum_mutual_information_explicit} at the bottom of page \pageref{eq:regularized_incomplete_gamma}, where
		\begin{equation}
			Q\Bigl(N,\frac{t_{l-1}}{\sigma_{m_{\mathcal{K}}}^2},\frac{t_l}{\sigma_{m_{\mathcal{K}}}^2}\Bigr) = \frac{\int_{{t_{l-1}}/{\sigma_{m_{\mathcal{K}}}^2}}^{{t_l}/{\sigma_{m_{\mathcal{K}}}^2}} z^{N-1} \exp(-z) \, d z}{(N-1)!}
			\label{eq:regularized_incomplete_gamma}
		\end{equation}
		is the regularized incomplete Gamma function that substitutes the \gls{dtmac} transition probability \eqref{eq:dtmac}.
		Its series representation is given by \cite[Theorem~3]{Jameson2016}
		\begin{align}
			Q\Bigl(N,\frac{t_{l-1}}{\sigma_{m_{\mathcal{K}}}^2},\frac{t_l}{\sigma_{m_{\mathcal{K}}}^2}\Bigr)
			 & = \exp \Bigl(-\frac{t_{l-1}}{\sigma_{m_{\mathcal{K}}}^2}\Bigr) \sum_{n=0}^{N-1} \frac{\Bigl(\frac{t_{l-1}}{\sigma_{m_{\mathcal{K}}}^2}\Bigr)^n}{n!} \nonumber \\
			 & \quad - \exp \Bigl(-\frac{t_l}{\sigma_{m_{\mathcal{K}}}^2}\Bigr) \sum_{n=0}^{N-1} \frac{\Bigl(\frac{t_l}{\sigma_{m_{\mathcal{K}}}^2}\Bigr)^n}{n!}.
			\label{eq:regularized_incomplete_gamma_series}
		\end{align}
		Next, we derive the gradients of \eqref{eq:regularized_incomplete_gamma_series} and \eqref{eq:weighted_sum_mutual_information_explicit} w.r.t. $\boldsymbol{w}^*$ as \eqref{eq:regularized_incomplete_gamma_gradient} and \eqref{eq:weighted_sum_mutual_information_gradient} at the end of page \pageref{eq:regularized_incomplete_gamma_gradient} and \pageref{eq:weighted_sum_mutual_information_gradient}, respectively.
		\begin{figure*}[!b]
			\begin{align}
				\nabla_{\boldsymbol{w}^*} Q\Bigl(N,\frac{t_{l-1}}{\sigma_{m_{\mathcal{K}}}^2},\frac{t_l}{\sigma_{m_{\mathcal{K}}}^2}\Bigr)
				 & = \frac{\boldsymbol{h}_{\mathrm{E}}(x_{m_{\mathcal{K}}})\boldsymbol{h}_{\mathrm{E}}^H(x_{m_{\mathcal{K}}})\boldsymbol{w}}{\bigl(\lvert\boldsymbol{h}_{\mathrm{E}}^H(x_{m_{\mathcal{K}}})\boldsymbol{w}\rvert^2+\sigma_v^2\bigr)^2}\nonumber                                                                                                                                                                                                        \\
				 & \quad \times \Biggl(t_l\exp\Bigl(-\frac{t_l}{\lvert\boldsymbol{h}_{\mathrm{E}}^H(x_{m_{\mathcal{K}}})\boldsymbol{w}\rvert^2+\sigma_v^2}\Bigr)\biggl(-1+\sum_{n=1}^{N-1} \frac{n\Bigl(\frac{t_l}{\lvert\boldsymbol{h}_{\mathrm{E}}^H(x_{m_{\mathcal{K}}})\boldsymbol{w}\rvert^2+\sigma_v^2}\Bigr)^{n-1}-\Bigl(\frac{t_l}{\lvert\boldsymbol{h}_{\mathrm{E}}^H(x_{m_{\mathcal{K}}})\boldsymbol{w}\rvert^2+\sigma_v^2}\Bigr)^n}{n!}\biggr)\nonumber    \\
				 & \qquad - t_{l-1}\exp\Bigl(-\frac{t_{l-1}}{\lvert\boldsymbol{h}_{\mathrm{E}}^H(x_{m_{\mathcal{K}}})\boldsymbol{w}\rvert^2+\sigma_v^2}\Bigr)\biggl(-1+\sum_{n=1}^{N-1} \frac{n\Bigl(\frac{t_{l-1}}{\lvert\boldsymbol{h}_{\mathrm{E}}^H(x_{m_{\mathcal{K}}})\boldsymbol{w}\rvert^2+\sigma_v^2}\Bigr)^{n-1}-\Bigl(\frac{t_{l-1}}{\lvert\boldsymbol{h}_{\mathrm{E}}^H(x_{m_{\mathcal{K}}})\boldsymbol{w}\rvert^2+\sigma_v^2}\Bigr)^n}{n!}\biggr)\Biggr)
				\label{eq:regularized_incomplete_gamma_gradient}
			\end{align}
		\end{figure*}
		\begin{figure*}[!b]
			\hrule
			\begin{align}
				\nabla_{\boldsymbol{w}^*} I(x_{\mathcal{K}})
				 & = \sum_{m_{\mathcal{K}}}P_{\mathcal{K}}(x_{m_{\mathcal{K}}})\Biggl(\rho\frac{\boldsymbol{h}_{\mathrm{E}}(x_{m_{\mathcal{K}}})\boldsymbol{h}_{\mathrm{E}}^H(x_{m_{\mathcal{K}}})\boldsymbol{w}}{\lvert\boldsymbol{h}_{\mathrm{E}}^H(x_{m_{\mathcal{K}}})\boldsymbol{w}\rvert^2+\sigma_v^2}+(1-\rho)\sum_l\biggl(\log\frac{Q\Bigl(N,\frac{t_{l-1}}{\sigma_{m_{\mathcal{K}}}^2},\frac{t_l}{\sigma_{m_{\mathcal{K}}}^2}\Bigr)}{\sum_{m_{\mathcal{K}}'}P_{\mathcal{K}}(x_{m_{\mathcal{K}}'})Q\Bigl(N,\frac{t_{l-1}}{\sigma_{m_{\mathcal{K}}'}^2},\frac{t_l}{\sigma_{m_{\mathcal{K}}'}^2}\Bigr)}+1\biggr)\nonumber   \\
				 & \qquad \times \nabla_{\boldsymbol{w}^*} Q\Bigl(N,\frac{t_{l-1}}{\sigma_{m_{\mathcal{K}}}^2},\frac{t_l}{\sigma_{m_{\mathcal{K}}}^2}\Bigr)-\frac{Q\Bigl(N,\frac{t_{l-1}}{\sigma_{m_{\mathcal{K}}}^2},\frac{t_l}{\sigma_{m_{\mathcal{K}}}^2}\Bigr)\sum_{m_{\mathcal{K}}'}P_{\mathcal{K}}(x_{m_{\mathcal{K}}'})\nabla_{\boldsymbol{w}^*}Q\Bigl(N,\frac{t_{l-1}}{\sigma_{m_{\mathcal{K}}'}^2},\frac{t_l}{\sigma_{m_{\mathcal{K}}'}^2}\Bigr)}{\sum_{m_{\mathcal{K}}'}P_{\mathcal{K}}(x_{m_{\mathcal{K}}'})Q\Bigl(N,\frac{t_{l-1}}{\sigma_{m_{\mathcal{K}}'}^2},\frac{t_l}{\sigma_{m_{\mathcal{K}}'}^2}\Bigr)}\Biggr)
				\label{eq:weighted_sum_mutual_information_gradient}
			\end{align}
		\end{figure*}
		% Next, we express the gradient of \eqref{eq:regularized_incomplete_gamma_series} and \eqref{eq:weighted_sum_mutual_information_explicit} w.r.t. $\boldsymbol{w}^*$ as \eqref{eq:regularized_incomplete_gamma_gradient} and \eqref{eq:weighted_sum_mutual_information_gradient} at the end of page \pageref{eq:regularized_incomplete_gamma_gradient},
		It allows problem \eqref{op:active_beamforming} to be solved by the \gls{pgd} method, where any unregulated beamforming vector $\bar{\boldsymbol{w}}$ can be projected onto the feasible domain of average transmit power constraint \eqref{co:transmit_power} by
		% the projection function associated with average transmit power constraint \eqref{co:transmit_power} is
		\begin{equation}
			\boldsymbol{w} = \sqrt{P} \frac{\bar{\boldsymbol{w}}}{\max\bigl(\sqrt{P},\lVert\bar{\boldsymbol{w}}\rVert\bigr)}.
			\label{eq:beamforming_projection}
		\end{equation}
		We present the iterative active beamforming design accelerated by backtracking line search in Algorithm \ref{al:active_beamforming}.
		\begin{algorithm}[!t]
			\caption{Iterative Active Beamforming Design by \gls{pgd} with Backtracking Line Search}
			\label{al:active_beamforming}
			\begin{algorithmic}[1]
				\Require $Q$, $N$, $\boldsymbol{h}_{\mathrm{D}}^H$, $\boldsymbol{H}_{\mathrm{C}}$, $\boldsymbol{\alpha}$, $\mathcal{X}$, $P$, $\sigma_v^2$, $\rho$, $\{\boldsymbol{p}_k\}_{k \in \mathcal{K}}$, $\boldsymbol{t}$, $\alpha$, $\beta$, $\gamma$, $\epsilon$
				\Ensure $\boldsymbol{w}^\star$
				\State Set $\boldsymbol{h}_{\mathrm{E}}^H(x_{m_{\mathcal{K}}})$, $\forall m_{\mathcal{K}}$ by \eqref{eq:equivalent_channel}
				\State \phantom{Set} $P_{\mathcal{K}}(x_{m_{\mathcal{K}}})$, $\forall m_{\mathcal{K}}$ by \eqref{eq:equivalent_distribution}
				\State Initialize $r \gets 0$
				\State \phantom{Initialize} $\boldsymbol{w}^{(0)}$, $\lVert\boldsymbol{w}^{(0)}\rVert^2 \le P$
				\State Get $(\sigma_{m_{\mathcal{K}}}^{(r)})^2$, $\forall m_{\mathcal{K}}$ by \eqref{eq:receive_variance} \label{st:gradient_descent_begin}
				\State \phantom{Get} $Q^{(r)}\bigl(N,\frac{t_{l-1}}{\sigma_{m_{\mathcal{K}}}^2},\frac{t_l}{\sigma_{m_{\mathcal{K}}}^2}\bigr)$, $\forall m_{\mathcal{K}},l$ by \eqref{eq:regularized_incomplete_gamma} or \eqref{eq:regularized_incomplete_gamma_series}
				\State \phantom{Get} $I^{(r)}(x_{\mathcal{K}})$ by \eqref{eq:weighted_sum_mutual_information_explicit} \label{st:gradient_descent_end}
				\State \phantom{Get} $\nabla_{\boldsymbol{w}^*} Q^{(r)}\bigl(N,\frac{t_{l-1}}{\sigma_{m_{\mathcal{K}}}^2},\frac{t_l}{\sigma_{m_{\mathcal{K}}}^2}\bigr)$, $\forall m_{\mathcal{K}},l$ by \eqref{eq:regularized_incomplete_gamma_gradient} \label{st:gradient_update_start}
				\State \phantom{Get} $\nabla_{\boldsymbol{w}^*} I^{(r)}(x_{\mathcal{K}})$ by \eqref{eq:weighted_sum_mutual_information_gradient} \label{st:gradient_update_end}
				\Repeat
					\State Update $r \gets r+1$
					\State \phantom{Update} $\gamma^{(r)}\gets\gamma$
					\State \phantom{Update} $\bar{\boldsymbol{w}}^{(r)} \gets \boldsymbol{w}^{(r-1)}+\gamma\nabla_{\boldsymbol{w}^*} I^{(r-1)}(x_{\mathcal{K}})$ \label{st:backtracking_line_search_begin}
					\State \phantom{Update} $\boldsymbol{w}^{(r)}$ by \eqref{eq:beamforming_projection}
					\State Redo step \ref{st:gradient_descent_begin}--\ref{st:gradient_descent_end} \label{st:backtracking_line_search_end}
					% \State \phantom{Update} $(\sigma_{m_{\mathcal{K}}}^{(r)})^2$, $\forall m_{\mathcal{K}}$ by \eqref{eq:receive_variance}
					% \State \phantom{Update} $Q^{(r)}\bigl(N,\frac{t_{l-1}}{\sigma_{m_{\mathcal{K}}}^2},\frac{t_l}{\sigma_{m_{\mathcal{K}}}^2}\bigr)$, $\forall m_{\mathcal{K}},l$ by \eqref{eq:regularized_incomplete_gamma} or \eqref{eq:regularized_incomplete_gamma_series}
					% \State \phantom{Update} $I^{(r)}(x_{\mathcal{K}})$ by \eqref{eq:weighted_sum_mutual_information_explicit} \label{st:backtracking_line_search_end}
					% \While{$I^{(r-1)}(x_{\mathcal{K}})>I^{(r)}(x_{\mathcal{K}})+\alpha\gamma\lVert\nabla_{\boldsymbol{w}^*}I^{(r-1)}(x_{\mathcal{K}})\rVert$}
					\While{$I^{(r)}(x_{\mathcal{K}})<I^{(r-1)}(x_{\mathcal{K}})+\alpha\gamma\lVert\nabla_{\boldsymbol{w}^*}I^{(r-1)}(x_{\mathcal{K}})\rVert^2$}
						\State Set $\gamma^{(r)}\gets\beta\gamma^{(r)}$
						\State Redo step \ref{st:backtracking_line_search_begin}--\ref{st:backtracking_line_search_end}
					\EndWhile
					\State Redo step \ref{st:gradient_update_start}, \ref{st:gradient_update_end}
				\Until $\lVert\boldsymbol{w}^{(r)}-\boldsymbol{w}^{(r-1)}\rVert \le \epsilon$
			\end{algorithmic}
		\end{algorithm}
	\end{subsection}

	\begin{subsection}{Decision Threshold}
		% Once node input distribution $\{\boldsymbol{p}_k\}_{k \in \mathcal{K}}$ and active beamforming vector $\boldsymbol{w}$ are obtained, problem \eqref{op:weighted_sum_rate}
		For any given $\{\boldsymbol{p}_k\}_{k \in \mathcal{K}}$ and $\boldsymbol{w}$, problem \eqref{op:weighted_sum_rate} reduces to
		\begin{maxi!}
			{\scriptstyle{\boldsymbol{t}\in\mathbb{R}_+^{L+1}}}{I(x_{\mathcal{K}})}{\label{op:decision_threshold}}{\label{ob:decision_threshold}}
			\addConstraint{\eqref{co:decision_threshold},}
		\end{maxi!}
		where it is trivial to conclude $t_0^\star=0$ and $t_L^\star=\infty$ for energy-based backscatter detection.

		\begin{remark}
			Backscatter detection (and decision design) has no impact on primary achievable rate.
			When nodes transmit at non-zero total rate, the user can re-encode backscatter messages to recover coded backscatter tuple $x_{\mathcal{K}}$ at each block.
			Otherwise, $x_{\mathcal{K}}$ can be fully deterministic and known to the user.
			% Once backscatter messages are successfully decoded, the user can re-encode backscatter messages to recover coded backscatter tuple $x_{\mathcal{K}}$ at each block.
			% Upon successful backscatter detection, the user can re-encode backscatter messages to recover coded backscatter tuple $x_{\mathcal{K}}$ at each block.
			% Therefore, decision design has no impact on primary achievable rate.
			% Decision threshold only influences backscatter rate instead of primary rate.
			\label{re:backscatter_detection}
		\end{remark}

		Remark \ref{re:backscatter_detection} suggests any $\boldsymbol{t}$ maximizes total backscatter mutual information $I^{\mathrm{B}}(x_{\mathcal{K}};\hat{x}_{\mathcal{K}})$ is also optimal for problem \eqref{op:decision_threshold}.

		\begin{remark}
			% With input distribution of all nodes, we can formulate an equivalent information source with augmented alphabet of tag input combination.
			% In terms of total backscatter rate, we can formulate an equivalent information source with augmented alphabet of tag input combination.
			% When input distribution of all nodes are available, we can formulate an equivalent information source in terms of total backscatter rate.
			% When input distribution of all nodes are available, we can think an equivalent information source with augmented alphabet of state tuple, in terms of total backscatter rate.
			% In terms of total backscatter rate, we can regard all nodes as an equivalent information source with augmented alphabet of node state tuple $x_{m_{\mathcal{K}}}$.
			In terms of total backscatter rate, the nodes can be regarded as an equivalent source with augmented alphabet of symbol tuple $x_{m_{\mathcal{K}}}$, and the \gls{dtmac} \eqref{eq:dtmac} essentially reduces to a \gls{dmtc}.
			% As such, the \gls{dtmac} \eqref{eq:dtmac} is essentially a \gls{dmtc}, and decision threshold design can be simplified accordingly.
		\end{remark}

		Finally, we can employ existing thresholding design for \gls{dmtc} to solve problem \eqref{op:decision_threshold}.
		For example, \cite{He2021} first discretized the continuous energy $z$ into numerous output bins, then grouped adjacent bins to maximize mutual information using \gls{dp} accelerated by \gls{smawk} algorithm.
		In \cite{Nguyen2020a}, the authors first proved the optimality condition for any three neighbor thresholds, then fix $t_0$, traverse $t_1$, and sequentially optimizes the others by bisection.
		Both will be compared with \gls{ml} decision \cite{Qian2019}
		\begin{equation}
			t_{l}^{\mathrm{\gls{ml}}} = N \frac{\sigma_{l-1}^2 \sigma_{l}^2}{\sigma_{l-1}^2 - \sigma_{l}^2} \log \frac{\sigma_{l-1}^2}{\sigma_{l}^2}, \quad l \in \mathcal{L} \setminus \{0,L\},
			\label{eq:detection_threshold_maximum_likelihood}
		\end{equation}
		which is generally suboptimal for problem \eqref{op:decision_threshold} except for equiprobable inputs at all nodes.

		% \begin{remark}
		% 	Although the \gls{ml} decision threshold is optimal in terms of the likelihood function, it is not necessarily capacity-achieving, although the performance gap can be negligible in the single-tag \gls{bibo} case.
		% \end{remark}

		% Interestingly, the optimal threshold design to maximize the mutual information for a general \gls{dmtc} with a fixed number of output letters remains an open issue.
		% The reason is that each decision region may contain more than one disjoint partitions (i.e., non-convex) and the number of thresholds are unknown.
		% Fortunately, for the proposed energy detection, we proved that the \gls{dmtc} capacity can be achieved using only convex decision regions.
		% This conclusion is summarized below.

		% \begin{proposition}
		% 	For a discrete-input continuous-output channel in Erlang distribution \eqref{eq:energy_distribution}, if the \gls{dmtc} is constructed for detection (i.e., same input/output alphabet) and $L$ input letters are with non-zero probability, then it is possible to achieve the \gls{dmtc} capacity by $L$ non-empty convex decision regions defined by $L+1$ distinct decision thresholds.
		% 	\label{pr:threshold}
		% \end{proposition}

		% \begin{proof}
		% 	Please refer to Appendix \ref{ap:threshold}.
		% 	\label{pf:threshold}
		% \end{proof}

		% Once the optimal number of decision threshold is determined, we can first discretize the output energy level into numerous bins, then obtain the optimal decision regions that maximizes the total backscatter rate by \gls{dp} accelerated by \gls{smawk} algorithm \cite{He2021}.
	\end{subsection}
\end{section}

\begin{section}{Simulation Results}
	% TODO add distribution to initializer to smooth curves?
	% TODO compare PGD and primary-MRT beamformers
	In this section, we provide numerical results to evaluate the proposed input, beamforming and decision design over a single-user multi-node Metascatter-enabled network.
	We assume the distance between \gls{ap} and user is \qty{10}{\meter}, and $K=2$ Metascatters are uniformly dropped within a disk centered at the user of radius \qty{1}{\meter}.
	% The carrier frequency is $f=\qty{200}{\MHz}$, and we consider \gls{iid} Ricean fading for all channels.
	The carrier frequency is $f=\qty{200}{\MHz}$, and we consider \gls{iid} Ricean fading between all terminals.
	For direct, forward and backward links, we set the path loss exponents to \num{2.6}, \num{2.4} and \num{2}, and the Ricean factor to \num{5}, \num{5} and \num{10}, respectively.
	% For direct, forward and backward links, we choose path loss exponents \num{2.6}, \num{2.4} and \num{2}, and Ricean factor \num{5}, \num{5} and \num{10}, respectively.
	% and the path loss exponents for direct, forward and backward links are \num{2.6}, \num{2.4} and \num{2}, respectively.
	% We consider \gls{iid} Ricean fading for all channels with Ricean factor set to \num{5}, \num{5} and \num{10}
	The \gls{ap} has $Q=4$ antennas with maximum average transmit power $P=\qty{36}{\dBm}$.
	All nodes have amplitude reflect ratio $\alpha=0.5$, symbol period ratio $N=20$, and perform \gls{qam} with $M=2$ input states.
	% and we consider amplitude reflect ratio $\alpha=0.5$ and symbol period ratio $N=20$.
	% The user has a receive antenna gain of \qty{3}{\dBi} and average noise power
	The user is with average noise power $\sigma_v^2=\qty{70}{\dBm}$ and receive antenna gain \qty{3}{\dBi}.
	All rate regions are averaged over at least \num{1000} instances, where ``PSP'' and ``BSP'' means primary and backscatter symbol periods, respectively.
	We choose decision design by \gls{smawk} \cite{He2021} as reference, and select discretize boundaries uniformly over the \qty{95}{\percent} confidence region of edge hypotheses.
	The parameters remain fixed unless otherwise specified.

	\begin{subsection}{Input Distribution vs. Weight}
		\begin{figure}[!t]
			\centering
			\subfloat[Instance 1\label{fi:distribution_weights_1}]{
				\resizebox{0.48\columnwidth}{!}{
					\input{assets/simulation/distribution_weights_1.tex}
				}
			}
			\subfloat[Instance 2\label{fi:distribution_weights_2}]{
				\resizebox{0.48\columnwidth}{!}{
					\input{assets/simulation/distribution_weights_2.tex}
				}
			}
			\caption{Typical input distributions vs. weight $\rho$ for a Metascatter with $M=4$ inputs.}
			\label{fi:distribution_weights}
		\end{figure}
		% We consider a single Metascatter with $M=4$ inputs in this case of study.
		In Fig. \ref{fi:distribution_weights}, we present typical input distributions of a single Metascatter with $M=4$ inputs under different weight $\rho$.
		Fig. \subref*{fi:distribution_weights_1} and \subref*{fi:distribution_weights_2} are obtained from \gls{iid} drop and channel realizations.
		We note that even for $\rho=0$ (i.e., best backscatter performance), the optimal input distribution is not equiprobable like \gls{sr}, and adaptive channel coding can further increase total backscatter rate based on \gls{csi}.
		On the other hand, the optimal input distribution becomes fully deterministic at $\rho=1$ (i.e., best primary performance), where the state maximizes equivalent primary channel \eqref{eq:equivalent_channel} strength is chosen with probability \num{1}.
		In this case, Metascatter boils down to a \gls{ris} element with $M$ discrete states.
		As $\rho$ moves from \num{0} to \num{1}, the optimal input distribution becomes gradually biased to one state and flexibly balances the primary-backscatter tradeoff.
	\end{subsection}

	\begin{subsection}{Rate Region vs. Input, Beamforming, and Decision Schemes}
		\begin{figure}[!t]
			\centering
			\subfloat[Input Distribution\label{fi:region_distribution}]{
				\resizebox{0.48\columnwidth}{!}{
					\input{assets/simulation/region_distribution.tex}
				}
			}
			\subfloat[Decision Threshold\label{fi:region_threshold}]{
				\resizebox{0.48\columnwidth}{!}{
					\input{assets/simulation/region_threshold.tex}
				}
			}
			\caption{Average primary-(total-)backscatter rate regions for different input distribution and decision threshold schemes.}
		\end{figure}
		Fig. \subref*{fi:region_distribution} compares the achievable rate regions by following input designs.
		\begin{itemize}
			% \item ``Exhaustion'' means $K$-dimensional grid search over probability simplex;
			% \item ``\gls{kkt}'' corresponds to Algorithm \ref{al:input_distribution};
			% \item ``Cooperation'' assumes backscatter cooperation/joint encoding at all Metascatters, and employs Blahut-Arimoto algorithm \cite{Arimoto1972,Blahut1972a} to determine \emph{tuple} input distribution;
			\item \textbf{Exhaustion:} $K$-dimensional grid search over probability simplex;
			\item \textbf{\gls{kkt}:} results of Algorithm \ref{al:input_distribution};
					% \item \textbf{Cooperation:} with backscatter cooperation/joint encoding at all Metascatters, Blahut-Arimoto algorithm \cite{Arimoto1972,Blahut1972a} for \emph{tuple} input distribution;
					% \item \textbf{Cooperation:} backscatter cooperation/joint encoding at all Metascatters, \emph{tuple input distribution} by Blahut-Arimoto algorithm \cite{Arimoto1972,Blahut1972a};
					% \item \textbf{Cooperation:} with backscatter cooperation (i.e., joint encoding) at all Metascatters, \emph{tuple input distribution} by Blahut-Arimoto algorithm \cite{Arimoto1972,Blahut1972a};
			\item \textbf{Cooperation:} backscatter cooperation/joint encoding at all Metascatters, tuple input distribution/joint probability array optimization by Blahut-Arimoto algorithm \cite{Arimoto1972,Blahut1972a};
					% with backscatter cooperation (i.e., joint encoding) at all Metascatters, \emph{tuple input distribution} by Blahut-Arimoto algorithm \cite{Arimoto1972,Blahut1972a};
			\item \textbf{Marginalization:} marginal distributions of joint probability array;
			\item \textbf{Decomposition:} normalized tensors of rank-1 \gls{cp} decomposition of joint probability array;
			\item \textbf{Randomization:} Gaussian recovery from joint probability array \cite{Calvo2010}.
		\end{itemize}

		We notice adaptive joint encoding at all Metascatters provides the outer bound of rate region.
		However, it involves arbitrarily dependent codewords, and backscatter cooperation between passive nodes is generally unaffordable.
		In contrast, the rate region of \gls{kkt} input design in Algorithm \ref{al:input_distribution} approaches that of exhaustive search, and the loss is negligible for $K=2$.
		% Although the randomization method \cite{Calvo2010} achieves similar performance, the computational complexity is much higher as it involves solving $K+1$ linear programming problems
		Although the randomization method \cite{Calvo2010} returns similar rate region, it requires solving $K+1$ linear programming problems before applying Gaussian recovery.
		The marginal distribution is slightly worse than \gls{kkt} despite having the same computational complexity, while the approximation from \gls{cp} decomposition is unsatisfying.

		% TODO: beamforming

		Fig. \subref*{fi:region_threshold} compares the achievable rate region by following threshold schemes.
		\begin{itemize}
			\item \textbf{\gls{dp}:} sequential quantizer grouping by dynamic programming \cite{He2021};
			\item \textbf{\gls{smawk}:} above accelerated by \gls{smawk} algorithm;
			\item \textbf{Bisection:} sequential bisection threshold design \cite{Nguyen2020a};
			\item \textbf{\gls{ml}:} maximum likelihood decision \eqref{eq:detection_threshold_maximum_likelihood} \cite{Qian2019}.
		\end{itemize}

		We observe that input-adaptive threshold designs achieve higher total backscatter rate than \gls{ml}.
		This is because the decision regions can be flexibly adjusted to enhance the capacity of \gls{dtmac} \eqref{eq:dtmac}.
		For example, the tuples with negligible input probability should have narrower decision regions than those frequently employed, in order to improve detection performance.
		It emphasizes the importance of joint input distribution and decision region design.

		% We note that ideal joint adaptive encoding at all Metascatters can further boost the total backscatter rate.
		% However, this ideal upper bound requires real-time feedback
		% ``Exhaustion'' means $K$-dimensional grid search over probability simplex, ``KKT'' corresponds to Algorithm \ref{ap:input_kkt_solution}, ``Cooperation'' assumes full backscatter cooperation and joint encoding at all Metascatters
	\end{subsection}


	\begin{subsection}{Rate Region vs. System Configuration}
		\begin{figure}[!t]
			\centering
			\subfloat[Metscatter Nodes\label{fi:region_tags}]{
				\resizebox{0.48\columnwidth}{!}{
					\input{assets/simulation/region_tags.tex}
				}
			}
			\subfloat[Input States\label{fi:region_states}]{
				\resizebox{0.48\columnwidth}{!}{
					\input{assets/simulation/region_states.tex}
				}
			}
			\\
			\subfloat[Transmit Antennas\label{fi:region_txs}]{
				\resizebox{0.48\columnwidth}{!}{
					\input{assets/simulation/region_txs.tex}
				}
			}
			\subfloat[Scatter Ratio\label{fi:region_scatter}]{
				\resizebox{0.48\columnwidth}{!}{
					\input{assets/simulation/region_scatter.tex}
				}
			}
			\caption{Average primary-(total-)backscatter rate regions.}
			\label{fi:region_config_1}
		\end{figure}

		% Fig. \ref{fi:region_config_1} reveals the impact of Metascatter nodes, input states, transmit antennas and scatter ratio on the achievable rate region.
		We present in \ref{fi:region_config_1} the impact of Metascatter nodes, input states, transmit antennas and scatter ratio on the achievable rate region.
		For the specified scenario, Fig. \subref*{fi:region_tags} shows the total backscatter rate almost scales proportionally with the number of Metascatter nodes, and the decrease of individual backscatter rate is unobvious.
		Besides, we conclude from \subref*{fi:region_states} that increasing the reflection states has a marginal effect on both primary and backscatter rates.
		Those two facts motivates the use of numerous elementary/uncomplicated backscatter nodes, instead of high-order transponders or programmable high-resolution surfaces.
		Figs. \subref*{fi:region_txs} and \subref*{fi:region_scatter} also prove increasing transmit antennas or scatter ratio can improve both primary and backscatter performance.

		\begin{figure}[!t]
			\centering
			\subfloat[Node Coverage\label{fi:region_coverage}]{
				\resizebox{0.48\columnwidth}{!}{
					\input{assets/simulation/region_coverage.tex}
				}
			}
			\subfloat[Symbol Period Ratio\label{fi:region_duration}]{
				\resizebox{0.48\columnwidth}{!}{
					\input{assets/simulation/region_duration.tex}
				}
			}
			\\
			\subfloat[Carrier Frequency\label{fi:region_frequency}]{
				\resizebox{0.48\columnwidth}{!}{
					\input{assets/simulation/region_frequency.tex}
				}
			}
			\subfloat[Average Noise Power\label{fi:region_noise}]{
				\resizebox{0.48\columnwidth}{!}{
					\input{assets/simulation/region_noise.tex}
				}
			}
			\caption{Average primary-(total-)backscatter rate regions.}
			\label{fi:region_config_2}
		\end{figure}

		Fig. \subref*{fi:region_coverage} shows the tradeoff between coverage disk radius $r$ and achievable rate region.
		When nodes are far from the user, both primary and backscatter rates decrease due to the product pass loss of forward and backward channels.
		Under the assumption of ideal backscatter decoding and re-encoding, Fig. \subref*{fi:region_duration} suggests using lower symbol period ratio $N$ can boost total backscatter rate per primary symbol.
		However, it requires more frequent detection and re-encoding at the user to maintain the primary rate.
		When $N$ becomes sufficiently large, total backscatter rate approaches \num{0} and Metascatters boil down to conventional \gls{ris} elements with fixed reflection patterns during whole channel block.
		In Fig. \subref*{fi:region_frequency}, we observe that passive Metascatter achieves higher backscatter rate at lower carrier frequency because of preferable propagation loss.
		Finally, \subref*{fi:region_noise} prove the performance of energy detection is robust for a wide range of noise power.
	\end{subsection}
\end{section}

\begin{section}{Conclusion}
	This paper introduced Metascatter that adapts input distribution of passive backscatter nodes to simultaneously transmit and assist over existing wireless systems.
	Starting from backscatter principles, we showed how Metascatters bridges and generalizes parasitic source of \gls{sr} and reflecting element of \gls{ris} via smart input design.
	An application scenario was considered where multiple Metascatters ride over a point-to-point transmission to simultaneously encode self message and perform passive beamforming.
	To characterize achievable primary-backscatter rate region, we proposed a \gls{bcd} algorithm that evaluates \gls{kkt} input distribution in closed form, optimizes active beamforming by \gls{pgd}, and refines decision regions by existing methods.
	Numerical results demonstrated the advantage of adaptive node input distribution design for both primary and backscatter subsystems.

	One particular interesting question is how to design Metascatters in a multi-user system.
	If one node can contribute to and be decoded by multiple users, its input distribution may be further adjusted to mimic multi-beam gain of dynamic beamforming \cite{Qiu2022}.
\end{section}

\begin{appendix}
	\begin{subsection}{Proof of Proposition \ref{pr:input_kkt_condition}}
		Denote the Lagrange multipliers associated with \eqref{co:sum_probability} and \eqref{co:nonnegative_probability} as $\{\nu_k\}_{k \in \mathcal{K}}$ and $\{\lambda_{k,m_k}\}_{k \in \mathcal{K},m_k \in \mathcal{M}}$, respectively.
		The Lagrangian function of problem \eqref{op:input_distribution} is
		\begin{align}
			% L(\{\boldsymbol{p}_k\}_{k \in \mathcal{K}},\{\nu_k\},\{\lambda_{k,m_k}\})
			L
			%  & = - I(x_{\mathcal{K}}) + \sum_{k \in \mathcal{K}} \nu_k \left( \sum_{m_k \in \mathcal{M}} P_k(x_{m_k}) - 1 \right)\nonumber \\
			 & = - I(x_{\mathcal{K}}) + \sum_k \nu_k \left( \sum_{m_k \in \mathcal{M}} P_k(x_{m_k}) - 1 \right)\nonumber \\
			%  & \quad - \sum_{k \in \mathcal{K}} \sum_{m_k \in \mathcal{M}} \lambda_{k,m_k} P_k(x_{m_k}),
			 & \quad - \sum_k \sum_{m_k} \lambda_{k,m_k} P_k(x_{m_k}),
		\end{align}
		% and the \gls{kkt} conditions on the optimal primal and dual variables are, $\forall m_k \in \mathcal{M}$ and $\forall k \in \mathcal{K}$,
		% and the \gls{kkt} conditions are, $\forall m_k \in \mathcal{M}$ and $\forall k \in \mathcal{K}$,
		and the \gls{kkt} conditions are, $\forall k,m_k$,
		\begin{subequations}
			\label{eq:input_kkt_condition_original}
			\begin{equation}
				- \nabla_{P_k^\star(x_{m_k})} I^\star(x_{\mathcal{K}}) + \nu_k^\star - \lambda_{k,m_k}^\star = 0,
			\end{equation}
			\begin{equation}
				\lambda_{k,m_k}^\star = 0, \quad P_k^\star(x_{m_k}) > 0,
			\end{equation}
			\begin{equation}
				\lambda_{k,m_k}^\star \ge 0, \quad P_k^\star(x_{m_k}) = 0.
			\end{equation}
		\end{subequations}
		The directional derivative can be explicitly expressed as
		\begin{equation}
			\nabla_{P_k^\star(x_{m_k})} I^\star(x_{\mathcal{K}}) = I_k^\star(x_{m_k}) - (1 - \rho).
			\label{eq:input_directional_derivative}
		\end{equation}
		Combining \eqref{eq:input_kkt_condition_original} and \eqref{eq:input_directional_derivative}, we have
		\begin{subequations}
			\label{eq:input_kkt_condition_transformed}
			\begin{alignat}{2}
				I_k^\star(x_{m_k}) & = \nu_k^\star + (1 - \rho), \quad   &  & P_k^\star(x_{m_k}) > 0,\label{eq:probable_states_marginal} \\
				I_k^\star(x_{m_k}) & \le \nu_k^\star + (1 - \rho), \quad &  & P_k^\star(x_{m_k}) = 0,\label{eq:dropped_states_marginal}
			\end{alignat}
		\end{subequations}
		which suggests
		\begin{equation}
			\sum_{m_k} P_k^\star(x_{m_k}) I_k^\star(x_{m_k}) = \nu_k^\star + (1 - \rho).
			\label{eq:input_kkt_condition_implied}
		\end{equation}
		On the other hand, by definition of weighted sum marginal information \eqref{eq:weighted_sum_marginal_information},
		\begin{equation}
			\sum_{m_k} P_k^\star(x_{m_k}) I_k^\star(x_{m_k}) = I^\star(x_{\mathcal{K}}),
			\label{eq:weighted_sum_marginal_information_implied}
		\end{equation}
		where the right-hand side is irrelevant to $k$.
		\eqref{eq:input_kkt_condition_transformed}, \eqref{eq:input_kkt_condition_implied}, and \eqref{eq:weighted_sum_marginal_information_implied} together complete the proof.
		\label{ap:input_kkt_condition}
	\end{subsection}

	\begin{subsection}{Proof of Proposition \ref{pr:input_kkt_solution}}
		We first prove sequence \eqref{eq:input_kkt_solution} is non-decreasing in weighted sum mutual information.
		Let $P_{\mathcal{K}}(x_{m_{\mathcal{K}}}) = \prod_{q \in \mathcal{K}} P_q(x_{m_q})$ and $P_{\mathcal{K}}'(x_{m_{\mathcal{K}}}) = P_k'(x_{m_k}) \prod_{q \in \mathcal{K} \setminus \{k\}} P_q(x_{m_q})$ be two probability distributions with potentially different marginal for tag $k \in \mathcal{K}$ at state $m_k \in \mathcal{M}$, and define an intermediate function $J \left( P_{\mathcal{K}}(x_{m_{\mathcal{K}}}),P_{\mathcal{K}}'(x_{m_{\mathcal{K}}}) \right)$ as \eqref{eq:intermediate_information_function} at the end of page \pageref{eq:intermediate_information_function}.
		\begin{figure*}[!b]
			\hrule
			% \begin{equation}
			% 	J \left( P_{\mathcal{K}}(x_{m_{\mathcal{K}}}),P_{\mathcal{K}}'(x_{m_{\mathcal{K}}}) \right) \triangleq \rho \sum_{m_{\mathcal{K}}} P_{\mathcal{K}}(x_{m_{\mathcal{K}}}) \log \left(1 + \frac{\lvert \boldsymbol{h}_{\mathrm{E}}^H(x_{m_{\mathcal{K}}}) \boldsymbol{w} \rvert^2}{\sigma_v^2}\right) + (1 - \rho) \sum_{m_{\mathcal{K}}} P_{\mathcal{K}}(x_{m_{\mathcal{K}}}) \sum_{m_{\mathcal{K}}'} P(\hat{x}_{m_{\mathcal{K}}'}|x_{m_{\mathcal{K}}}) \log \frac{P(\hat{x}_{m_{\mathcal{K}}'}|x_{m_{\mathcal{K}}}) P_{\mathcal{K}}'(x_{m_{\mathcal{K}}})}{P'(\hat{x}_{m_{\mathcal{K}}'}) P_{\mathcal{K}}(x_{m_{\mathcal{K}}})}.
			% 	\label{eq:intermediate_information_function}
			% \end{equation}
			\begin{align}
				J \left( P_{\mathcal{K}}(x_{m_{\mathcal{K}}}),P_{\mathcal{K}}'(x_{m_{\mathcal{K}}}) \right)
				 & \triangleq \rho \sum_{m_{\mathcal{K}}} P_{\mathcal{K}}(x_{m_{\mathcal{K}}}) \log \left(1 + \frac{\lvert \boldsymbol{h}_{\mathrm{E}}^H(x_{m_{\mathcal{K}}}) \boldsymbol{w} \rvert^2}{\sigma_v^2}\right)\nonumber                                                                                                                    \\
				 & \quad + (1 - \rho) \sum_{m_{\mathcal{K}}} P_{\mathcal{K}}(x_{m_{\mathcal{K}}}) \sum_{m_{\mathcal{K}}'} P(\hat{x}_{m_{\mathcal{K}}'}|x_{m_{\mathcal{K}}}) \log \frac{P(\hat{x}_{m_{\mathcal{K}}'}|x_{m_{\mathcal{K}}}) P_{\mathcal{K}}'(x_{m_{\mathcal{K}}})}{P'(\hat{x}_{m_{\mathcal{K}}'}) P_{\mathcal{K}}(x_{m_{\mathcal{K}}})}.
				\label{eq:intermediate_information_function}
			\end{align}
		\end{figure*}
		It is straightforward to verify $J \left( P_{\mathcal{K}}(x_{m_{\mathcal{K}}}),P_{\mathcal{K}}(x_{m_{\mathcal{K}}}) \right) = I(x_{\mathcal{K}})$ and $J \left( P_{\mathcal{K}}(x_{m_{\mathcal{K}}}),P_{\mathcal{K}}'(x_{m_{\mathcal{K}}}) \right)$ is a concave function for a fixed $P_{\mathcal{K}}'(x_{m_{\mathcal{K}}})$.
		By choosing $\nabla_{P_k^\star(x_{m_k})} J \left( P_{\mathcal{K}}(x_{m_{\mathcal{K}}}),P_{\mathcal{K}}'(x_{m_{\mathcal{K}}}) \right) = 0$, we have
		\begin{equation}
			S_k'(x_{m_k}) - S_k'(x_{i_k}) + (1 - \rho) \log \frac{P_k(x_{i_k})}{P_k^\star(x_{m_k})} = 0,
			\label{eq:optimal_intermediate_information_condition}
		\end{equation}
		where $i_k \ne m_k$ is the reference state and
		\begin{align}
			S_k'(x_{m_k})
			 & \triangleq I_k'(x_{m_k}) + (1 - \rho) \sum_{m_{\mathcal{K} \setminus \{k\}}} P_{\mathcal{K} \setminus \{k\}}(x_{m_{\mathcal{K} \setminus \{k\}}})\nonumber \\
			 & \quad \times \sum_{m_{\mathcal{K}}'} P(\hat{x}_{m_{\mathcal{K}}'}|x_{m_{\mathcal{K}}}) \log P_{\mathcal{K}}'(x_{m_{\mathcal{K}}}).
		\end{align}
		Evidently, $\forall m_k \ne i_k$, \eqref{eq:optimal_intermediate_information_condition} boils down to
		\begin{equation}
			P_k^\star(x_{m_k}) = \frac{P_k'(x_{m_k}) \exp \left( \frac{\rho}{1 - \rho} I_k'(x_{m_k}) \right)}{\sum_{m_k'} P_k'(x_{m_k'}) \exp \left( \frac{\rho}{1 - \rho} I_k'(x_{m_k'}) \right)}.
			\label{eq:optimal_relative_distribution}
		\end{equation}
		We also notice $P_k(x_{i_k}) = 1 - \sum_{m_k \ne i_k} P_k^\star(x_{m_k})$ has exactly the same expression as \eqref{eq:optimal_relative_distribution}.
		% We also notice $P_k(x_{i_k}) = 1 - \sum_{m_k \ne i_k} P_k^\star(x_{m_k})$ is in the same expression as \eqref{eq:optimal_relative_distribution}.
		Therefore, the result is irrelevant to the choice of reference state, and \eqref{eq:optimal_relative_distribution} is indeed optimal $\forall m_k \in \mathcal{M}$.
		% It implies the selection of reference state does not matter and \eqref{eq:optimal_relative_distribution} is indeed optimal $\forall m_k \in \mathcal{M}$.
		That is, for a fixed $P_{\mathcal{K}}'(x_{m_{\mathcal{K}}})$, choosing $P_k(x_{m_k})$ by \eqref{eq:optimal_relative_distribution} ensures
		\begin{equation}
			J \left( P_{\mathcal{K}}(x_{m_{\mathcal{K}}}),P_{\mathcal{K}}'(x_{m_{\mathcal{K}}}) \right) \ge I'(x_{\mathcal{K}}).
			\label{eq:information_difference_lower}
		\end{equation}
		On the other hand, it also guarantees
		\begin{subequations}
			\label{eq:information_difference_upper}
			\begin{align}
				\Delta
				 & \triangleq I(x_{\mathcal{K}}) - J \left( P_{\mathcal{K}}(x_{m_{\mathcal{K}}}),P_{\mathcal{K}}'(x_{m_{\mathcal{K}}}) \right)                                                                                \\
				 & = (1 - \rho) \sum_{m_k} \frac{P_k'(x_{m_k}) f_k'(x_{m_k})}{\sum_{m_k'} P_k'(x_{m_k'}) f_k'(x_{m_k'})} \sum_{m_{\mathcal{K}}''} P(\hat{x}_{m_{\mathcal{K}}''}|x_{m_k})\nonumber                             \\
				 & \quad \times \log \frac{\sum_{m_k'} P_k'(x_{m_k'}) P(\hat{x}_{m_{\mathcal{K}}''}|x_{m_k'}) f_k'(x_{m_k})}{\sum_{m_k'} P_k'(x_{m_k'}) P(\hat{x}_{m_{\mathcal{K}}''}|x_{m_k'}) f_k'(x_{m_k'})}               \\
				 & \ge (1 - \rho) \sum_{m_k} \frac{P_k'(x_{m_k}) f_k'(x_{m_k})}{\sum_{m_k'} P_k'(x_{m_k'}) f_k'(x_{m_k'})} \sum_{m_{\mathcal{K}}''} P(\hat{x}_{m_{\mathcal{K}}''}|x_{m_k})\nonumber                           \\
				 & \quad \times \left( 1 - \frac{\sum_{m_k'} P_k'(x_{m_k'}) P(\hat{x}_{m_{\mathcal{K}}''}|x_{m_k'}) f_k'(x_{m_k'})}{\sum_{m_k'} P_k'(x_{m_k'}) P(\hat{x}_{m_{\mathcal{K}}''}|x_{m_k'}) f_k'(x_{m_k})} \right) \\
				 & = 0,
			\end{align}
		\end{subequations}
		where $f_k'(x_{m_k}) \triangleq \exp \left( \frac{\rho}{1 - \rho} I_k'(x_{m_k}) \right)$ and the equality holds if and only if \eqref{eq:optimal_relative_distribution} converges.
		\eqref{eq:information_difference_lower} and \eqref{eq:information_difference_upper} together imply $I(x_{\mathcal{K}}) \ge I'(x_{\mathcal{K}})$.
		Since mutual information is bounded above, we conclude the sequence \eqref{eq:input_kkt_solution} is non-decreasing and convergent in mutual information.

		Next, we prove any converging point of sequence \eqref{eq:input_kkt_solution}, denoted as $P_k^\star(x_{m_k})$, fulfills \gls{kkt} conditions \eqref{eq:input_kkt_condition}.
		% Next, we prove that sequence \eqref{eq:input_kkt_solution} fulfills \gls{kkt} conditions \eqref{eq:input_kkt_condition} on convergence.
		% To see this, recall $P_k^{(0)}(x_{m_k}) > 0$ and define
		To see this, define
		% To see this, we define
		\begin{equation}
			D_k^{(r)}(x_{m_k}) \triangleq \frac{P_k^{(r+1)}(x_{m_k})}{P_k^{(r)}(x_{m_k})} = \frac{f_k^{(r)}(x_{m_k})}{\sum_{m_k'} P_k^{(r)}(x_{m_k'}) f_k^{(r)}(x_{m_k'})}.
		\end{equation}
		As sequence \eqref{eq:input_kkt_solution} is convergent, any state with $P_k^\star(x_{m_k}) > 0$ need to satisfy $D_k^\star(x_{m_k}) \triangleq \lim_{r \to \infty} D_k^{(r)}(x_{m_k}) = 1$, namely
		\begin{equation}
			I_k^\star(x_{m_k}) = \frac{1 - \rho}{\rho} \log \sum_{m_k'} P_k^\star(x_{m_k'}) f_k^\star(x_{m_k'}),
		\end{equation}
		which is reminiscent of \eqref{eq:probable_states_marginal} and \eqref{eq:probable_states}.
		That is to say, given $P_k^{(0)}(x_{m_k}) > 0$, any converging point with $P_k^\star(x_{m_k}) > 0$ must satisfy \eqref{eq:probable_states}.
		On the other hand, we assume $P_k^\star(x_{m_k})$ does not satisfy \eqref{eq:dropped_states}, such that for any state with $P_k^\star(x_{m_k}) = 0$,
		% On the other hand, we show if $P_k^\star(x_{m_k})$ does not satisfy \eqref{eq:dropped_states}, then it is not a converging point of sequence \eqref{eq:input_kkt_solution}. By this assumption, for any state with $P_k^\star(x_{m_k}) = 0$,
		\begin{equation}
			I_k^\star(x_{m_k}) > I^\star(x_{\mathcal{K}}) = \sum_{m_k'} P_k^\star(x_{m_k'}) I_k^\star(x_{m_k'}),
		\end{equation}
		where the equality inherits from \eqref{eq:weighted_sum_mutual_information}.
		Since exponential function is monotonically increasing, we have $f_k^\star(x_{m_k}) > \sum_{m_k'} P_k^\star(x_{m_k'}) f_k^\star(x_{m_k'})$ and $D_k^\star(x_{m_k}) > 1$.
		Considering $P_k^{(0)}(x_{m_k}) > 0$ and $P_k^\star(x_{m_k}) = 0$, it contradicts with
		\begin{equation}
			P_k^{(r)}(x_{m_k}) = P_k^{(0)}(x_{m_k}) \prod_{n=1}^r D_k^{(n)}(x_{m_k}).
		\end{equation}
		Therefore, given $P_k^{(0)}(x_{m_k}) > 0$, any converging point with $P_k^\star(x_{m_k}) = 0$ must satisfy \eqref{eq:dropped_states}.
		This completes the proof.
		\label{ap:input_kkt_solution}
	\end{subsection}

	% \begin{subsection}{Proof of Proposition \ref{pr:threshold}}
	% 	Since $L$ input letters are with non-zero probability and $x \to z \to \hat{x}$ formulates a Markov chain, we need $L$ non-empty decision regions and at least $L+1$ distinct thresholds (including \num{0} and $\infty$) to minimize the distortion between source and decision.
	% 	On the other hand, the optimal decision regions are apparently empty for those unused letters.

	% 	Suppose the optimal number of thresholds is $S+1$ where $S \ge L$.
	% 	Let $\boldsymbol{t} \triangleq [t_0,\ldots,t_S]^T \in \mathbb{R}_{+}^{(S+1) \times 1}$ be the optimal threshold vector where $t_{s-1} < t_s$, $\forall s \in \mathcal{S} \triangleq \{1,\ldots,S\}$.
	% 	Since the optimal decision region for any letter may consist of multiple partitions, without loss of generality, we assume the mapping from threshold vector to decision region $l' \in \mathcal{L} \triangleq \{1,\ldots,L\}$ is given by $\mathcal{R}_{l'} = \bigcup_{s \equiv l' \pmod L} [t_{s-1},t_s)$.
	% 	\begin{footnote}
	% 		The proof holds for any valid mapping from threshold vector to decision regions, and we consider this specific case for the ease of presentation.
	% 	\end{footnote}
	% 	The threshold optimization problem is
	% 	\begin{maxi!}
	% 		{\scriptstyle{\boldsymbol{t}}}{I_{\mathrm{B}}(x;\hat{x})}{\label{op:decision_threshold}}{\label{ob:backscatter_mutual_information}}
	% 		\addConstraint{t_{s-1}}{< t_s,}{\quad \forall s \in \mathcal{S}.}{\label{co:strict_inequality}}
	% 	\end{maxi!}

	% 	Problem \eqref{op:decision_threshold} is intricate due to the strict inequality constraint \eqref{co:strict_inequality}.
	% 	Following \cite{Nguyen2020}, we first relax it to the convex counterpart, then discard the solutions that violate any original constraint.
	% 	The Lagrangian function for the relaxed problem is
	% 	\begin{equation}
	% 		L = - I_{\mathrm{B}}(x;\hat{x}) + \sum_{s \in \mathcal{S}} \mu_s (t_{s-1} - t_s),
	% 	\end{equation}
	% 	where $\mu_s$ is the Lagrange multiplier associated with the non-strict version of \eqref{co:strict_inequality}.
	% 	The \gls{kkt} conditions on the optimal primal and dual solutions are, $\forall s \in \mathcal{S}$,
	% 	\begin{subequations}
	% 		\label{eq:kkt_thresholding}
	% 		\begin{equation}
	% 			- \nabla_{t_s^\star} I^\star_{\mathrm{B}}(x;\hat{x}) + \mu_{s-1}^\star - \mu_s^\star = 0,
	% 			\label{eq:stationarity}
	% 		\end{equation}
	% 		\begin{equation}
	% 			\mu_s^\star \ge 0,
	% 			\label{eq:dual_feasibility}
	% 		\end{equation}
	% 		\begin{equation}
	% 			\mu_s^\star (t_{s-1}^\star - t_s^\star) = 0.
	% 			\label{eq:complementary_slackness}
	% 		\end{equation}

	% 	\end{subequations}
	% 	Due to the strict inequality constraint \eqref{co:strict_inequality}, conditions \eqref{eq:dual_feasibility} and \eqref{eq:complementary_slackness} together imply $\mu_s^\star = 0$, $\forall s \in \mathcal{S}$.
	% 	Besides, it is trivial to conclude $t_0^\star = 0$ for energy-based detection.
	% 	As such, the necessary optimality conditions for problem \eqref{op:decision_threshold}, $\forall s \in \mathcal{S}$,
	% 	\begin{equation}
	% 		\nabla_{t_{s}^\star} I^\star_{\mathrm{B}}(x;\hat{x}) = 0,
	% 	\end{equation}
	% 	which can be explicitly written as, $\forall s \equiv l' \pmod L$,
	% 	\begin{equation}
	% 		\sum_l P(x_l) \frac{(t_s^\star)^{N-1} \exp{-t_s^\star/\sigma_l^2}}{\sigma_l^{2N} (N-1)!} \log \frac{P(x_l|\hat{x}_{l'+1})}{P(x_l|\hat{x}_{l'})} = 0,
	% 		\label{eq:kkt_threshold_explicit}
	% 	\end{equation}

	% 	According to \cite{He2021}, the optimal backward channel quantizer is convex and separates each pair of posterior distribution by a hyperplane.
	% 	It implies, for a given output letter $l'$, the sequence $\{\log {P(x_l|\hat{x}_{l'+1})}/{P(x_l|\hat{x}_{l'})}\}_{l \in \mathcal{L}}$ changes sign exactly once.
	% 	We notice the left-hand side of \eqref{eq:kkt_threshold_explicit} is a generalized Dirichlet polynomial, and by Descartes' rule of signs \cite{Jameson2006}, has at most one positive solution.
	% 	% TODO
	% 	In other words, starting from $t_0^\star$, each optimal decision region requires at most one additional distinct threshold, and we have $S \le L$.
	% 	Therefore, we conclude $S = L$ and the proof is completed.
	% 	\qedsymbol
	% 	\label{ap:threshold}
	% \end{subsection}
\end{appendix}


\bibliographystyle{IEEEtran}
\bibliography{library.bib}
\end{document}
