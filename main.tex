\documentclass[journal]{IEEEtran}

\usepackage{adjustbox}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{array}
\usepackage{circuitikz}
\usepackage{cite}
\usepackage{colortbl}
\usepackage{environ}
\usepackage{grffile}
\usepackage{hyperref}
\usepackage{import}
\usepackage{mathtools}
\usepackage{microtype}
\usepackage{multirow}
\usepackage{pgffor}
\usepackage{pgfplots}
\usepackage{physics}
\usepackage{siunitx}
\usepackage{stfloats}
\usepackage{tikz}
\usepackage{url}
\usepackage{xcolor}
\usepackage[acronym]{glossaries-extra}
\usepackage[T1]{fontenc}
\usepackage[caption=false,font=footnotesize,subrefformat=parens,labelformat=parens]{subfig}
\usepackage[short]{optidef}
% \usepackage[subtle]{savetrees}

\listfiles
\interdisplaylinepenalty=2500
\pgfplotsset{compat=newest}
\usepgfplotslibrary{patchplots}
\newtheorem{proposition}{Proposition}
\newtheorem{remark}{Remark}
\newtheorem{theorem}{Theorem}
\DeclareSIUnit{\belm}{Bm}
\DeclareSIUnit{\dBm}{\deci\belm}
\DeclareSIUnit{\beli}{Bi}
\DeclareSIUnit{\dBi}{\deci\beli}
\ctikzset{american}
\usetikzlibrary{arrows,calc,matrix,patterns,positioning}

\DeclarePairedDelimiterX{\infdivx}[2]{(}{)}{%
	#1\;\delimsize\|\;#2%
}
\newcommand{\infdiv}{D\infdivx}

\makeatletter
\tikzset{
	block/.style={draw,rectangle,align=center},
    from/.style args={#1 to #2}{
        above right={0cm of #1},
        /utils/exec=\pgfpointdiff
            {\tikz@scan@one@point\pgfutil@firstofone(#1)\relax}
            {\tikz@scan@one@point\pgfutil@firstofone(#2)\relax},
        minimum width/.expanded=\the\pgf@x,
        minimum height/.expanded=\the\pgf@y
	}
}
\makeatother

\algrenewcommand{\algorithmicwhile}{\textbf{While}}
\algrenewcommand{\algorithmicif}{\textbf{If}}
\algrenewcommand{\algorithmicthen}{\textbf{Then}}
\algrenewcommand{\algorithmicelse}{\textbf{Else}}
\algrenewcommand{\algorithmicend}{\textbf{End}}
\algrenewcommand{\algorithmicrepeat}{\textbf{Repeat}}
\algrenewcommand{\algorithmicuntil}{\textbf{Until}}

\setabbreviationstyle[acronym]{long-short}

\newacronym{ambc}{AmBC}{Ambient Backscatter Communications}
\newacronym{ap}{AP}{Access Point}
\newacronym{awgn}{AWGN}{Additive White Gaussian Noise}
\newacronym{bcd}{BCD}{Block Coordinate Descent}
\newacronym{bibo}{BIBO}{Binary-Input Binary-Output}
\newacronym{bpcu}{\si{bpcu}}{bit per channel use}
\newacronym{bpsphz}{\si{bps/Hz}}{bit per second per Hertz}
\newacronym{cscg}{CSCG}{Circularly Symmetric Complex Gaussian}
\newacronym{csi}{CSI}{Channel State Information}
\newacronym{dmc}{DMC}{Discrete Memoryless Channel}
\newacronym{dmtc}{DMTC}{Discrete Memoryless Thresholding Channel}
\newacronym{irs}{IRS}{Intelligent Reflecting Surface}
\newacronym{kkt}{KKT}{Karush–Kuhn–Tucker}
\newacronym{mac}{MAC}{Multiple Access Channel}
\newacronym{mc}{MC}{Multiplication Coding}
\newacronym{ml}{ML}{Maximum-Likelihood}
\newacronym{psk}{PSK}{Phase Shift Keying}
\newacronym{qam}{QAM}{Quadrature Amplitude Modulation}
\newacronym{rf}{RF}{Radio Frequency}
\newacronym{sc}{SC}{Superposition Coding}
\newacronym{sic}{SIC}{Successive Interference Cancellation}
\newacronym{sr}{SR}{Symbiotic Radio}
\newacronym{tg}{TG}{tag}
\newacronym{ue}{UE}{user}



\begin{document}
	\title{Backscatter Modulation Design for Symbiotic Radio Networks}
	\maketitle

	\begin{section}{Backscatter Model}
		\begin{subsection}{Backscatter Principles}
			\begin{figure}[!t]
				\centering
				\subfloat[Block diagram of a passive tag]{
					\resizebox{0.6\columnwidth}{!}{
						\begin{circuitikz}[transform shape]
							\coordinate (O) at (0,0);
							\node[block,from={O to $(O) + (6.375,3.25)$}](T){};
							\draw (0,1.75)
								to[short] ++(-1,0)
								to[short] ++(0,0.75) node[bareantenna](A){Ant};
							\draw (0,1.75)
								to[short,-*] ++(0.25,0) coordinate(J);
							\draw (J)
								to[short] ++(0,1)
								to[short] ++(0.5,0) coordinate(J1);
							\node[block,from={$(J1) + (0,-0.25)$ to $(J1) + (2.5,0.25)$}](R){Rectifier};
							\draw (J)
								to[short] ++(0.5,0) coordinate(J2);
							\node[block,from={$(J2) + (0,-0.25)$ to $(J2) + (2.5,0.25)$}](D){Demodulator};
							\draw (J)
								to[short] ++(0,-1)
								to[short] ++(0.5,0) coordinate(J3)
								to [vR,mirror,invert,/tikz/circuitikz/bipoles/length=1cm] ++(1.75,0) node[ground,rotate=90]{};
							\node[block,from={$(J3) + (0,-0.25)$ to $(J3) + (2.5,0.25)$},draw=none](M){};
							\draw ($(J3) + (0,-0.25)$) rectangle ($(J3) + (2.5,0.25)$);
							\draw ($(J3) + (1.25,-0.5)$) node[]{Modulator};
							\draw[-{Latex[length=2mm]}] (R.east) -- ++(0.75,0);
							\draw[-{Latex[length=2mm]}] (D.east) -- ++(0.75,0);
							\draw[{Latex[length=2mm]}-] (M.east) -- ++(0.75,0);
							\node[block,from={$(R.east) + (0.75,-0.25)$ to $(R.east) + (2.875,0.25)$}](P){Power buffer};
							\node[block,from={$(M.east) + (0.75,-0.25)$ to $(D.east) + (2.875,0.25)$}](S){Digital\\section};
							\draw[dashed,-{Latex[length=2mm]}] (P.south) to (S.north);
							\coordinate (F1) at ($(P.south)!0.5!(S.north)$);
							\coordinate (F2) at ($(D.east)!0.5!(S.west)$);
							\coordinate (F3) at ($(D.south)!0.5!(M.north)$);
							\draw[dashed] (F1) to (F1-|D.north);
							\draw[dashed,-{Latex[length=2mm]}] (F1-|D.north) to (D.north);
							\draw[dashed] (F1-|F2) to (F2|-F3) to (M|-F3);
							\draw[dashed,-{Latex[length=2mm]}] (M|-F3) to (M.north);
						\end{circuitikz}
					}
					\label{fi:block_diagram_of_a_passive_tag}
				}
				\subfloat[Backscatter modualtion]{
					\resizebox{0.35\columnwidth}{!}{
						\begin{circuitikz}[transform shape]
							\draw (0,0) node[bareantenna](bareantenna){};
							\draw (bareantenna.west) ++(-1.5,0) node[waves](WI){};
							\draw (WI.north east) ++(0.25,0) node[font=\Large]{$\vec{E}_{\mathrm{I}}$};
							\draw (WI.south east) ++(0.25,0) node[font=\Large]{$\vec{H}_{\mathrm{I}}$};
							\draw (bareantenna.east) ++(0.875,0) node[waves](WR){};
							\draw (WR.north east) ++(0.25,0) node[font=\Large]{$\vec{E}_m$};
							\draw (WR.south east) ++(0.25,0) node[font=\Large]{$\vec{H}_m$};
							\draw (bareantenna)
								to [R=$Z_{\mathrm{A}}$,font=\Large,/tikz/circuitikz/bipoles/length=1cm] ++(0,-2)
								to [R=$Z_m$,font=\Large,/tikz/circuitikz/bipoles/length=1cm] ++(2,0) node[ground]{};
						\end{circuitikz}
					}
					\label{fi:backscatter_modualtion}
				}
				\caption{For a passive tag, the rectifier and demodulator rely on the incident electromagnetic wave for energy harvesting and source decoding, while the load-switcher adjusts the reradiated signal for backscatter modulation.}
				\label{fi:tag}
			\end{figure}
			A bistatic backscatter system consists of an excitation source, multiple (semi-)passive tags, and a backscatter reader. When illuminated, the tags simultaneously harvest energy, backscatter message, and demodulate the source signal if necessary. Fig.~\subref*{fi:block_diagram_of_a_passive_tag} shows a typical passive with a scattering antenna, an energy harvester, an integrated receiver\footnote{For example, \cite{Kim2021a} prototyped a compact-size pulse position demodulator based on an envelope detector, which brings great potential to coordination, synchronization, and reflection pattern control.}, a load-switching modulator, and on-chip components (e.g., micro-controller, memory, and sensors). A portion of the impinging signal is absorbed by the tag while the remaining is backscattered to the space. According to Green's decomposition \cite{Hansen1989}, the backscattered signal can be decomposed into the \emph{structural mode} component and the \emph{antenna mode} component. The former is fixed and depends on the antenna geometry and material properties\footnote{The structural mode component can be regarded as part of the environment multipath and modeled by channel estimation \cite{Boyer2014}.}, while the latter is adjustable and depends on the mismatch of the antenna and load impedance. Fig.~\subref*{fi:backscatter_modualtion} illustrates a simplified circuit and backscatter model at tag state $m$. The corresponding reflection coefficient is defined as\footnote{We assume the linear backscatter model where the reflection coefficient is irrelevant to the incident electromagnetic field at the tag \cite{Dobkin2012}.}
			\begin{equation}
				\Gamma_m = \frac{Z_m - Z_{\mathrm{A}}^*}{Z_m + Z_{\mathrm{A}}},
				\label{eq:reflection_coefficient}
			\end{equation}
			where $Z_m$ is the load impedance at state $m$ and $Z_{\mathrm{A}}$ is the antenna input impedance.
		\end{subsection}

		\begin{subsection}{Backscatter Modulation}
			Backscatter modulation is achieved by switching the tag load impedance between different states. For an $M$-ary \gls{qam} at state $m \in \mathcal{M} \triangleq \{1,\ldots,M\}$, the reflection coefficient $\Gamma_m$ maps to the signal constellation point $\bar{c}_m$ as \cite{Thomas2012a}
			\begin{equation}
				\Gamma_m = \alpha \frac{\bar{c}_m}{\max_{m'} \lvert \bar{c}_{m'} \rvert},
				\label{eq:backscatter_modulation}
			\end{equation}
			where $\alpha \in [0,1]$ is the reflection efficiency at a given direction. For simplicity, we consider an $M$-ary \gls{psk} with constellation set $\mathcal{C} \triangleq \{\bar{c}_1,\ldots,\bar{c}_M\}$, where the $m$-th constellation point is
			\begin{equation}
				\bar{c}_m = \exp \left(j \frac{2 \pi m}{M}\right).
				\label{eq:mpsk}
			\end{equation}

			\begin{remark}
				For passive tags, the reflection efficiency $\alpha$ controls the tradeoff between the harvestable power and backscatter strength: $\alpha = 0$ corresponds to maximum power transfer to the tag, while $\alpha = 1$ with $M$-\gls{psk} corresponds to ideal \gls{irs} with $M$ discrete states.
			\end{remark}
		\end{subsection}
	\end{section}

	\begin{section}{Backscatter Detection and Achievable Rates}
		\begin{subsection}{System Model}
			\begin{figure}[!t]
				\centering
				\def\svgwidth{0.9\columnwidth}
				\import{assets/}{symbiotic_radio.eps_tex}
				\caption{A single-user multi-tag symbiotic radio system.}
				\label{fi:symbiotic_radio}
			\end{figure}
			As shown in Fig.~\ref{fi:symbiotic_radio}, we propose a single-\gls{ue} multi-\gls{tg} symbiotic radio network where the \gls{rf} signal generated by the $Q$-antenna \gls{ap} is shared by two coexisting systems. In the primary downlink system, the \gls{ap} transmits information to the single-antenna user. In the secondary backscatter system, the \gls{ap} acts as the carrier emitter, the $K$ nearby single-antenna tags modulate their information over the reradiated \gls{rf} signals, and the user serves as the multi-tag backscatter reader. Denote the \gls{ap}-\gls{ue} direct channel as $\boldsymbol{h}_{\mathrm{D}}^H \in \mathbb{C}^{1 \times Q}$, the \gls{ap}-\gls{tg} $k \in \mathcal{K} \triangleq \{1,\ldots,K\}$ forward channel as $\boldsymbol{h}_{\mathrm{F},k}^H \in \mathbb{C}^{1 \times Q}$, the \gls{tg} $k$-\gls{ue} backward channel as $h_{\mathrm{B},k}$, and the cascaded forward-backward channel of tag $k$ as $\boldsymbol{h}_{\mathrm{C},k}^H \triangleq h_{\mathrm{B},k} \boldsymbol{h}_{\mathrm{F},k}^H \in \mathbb{C}^{1 \times Q}$. For simplicity, we consider a quasi-static block fading model where the channel coefficients remain constant within each coherence interval and vary independently over different coherence intervals, and assume the coherence interval $T$ is much longer than the backscatter symbol period $T_c$ and primary symbol period $T_s$. We also assume the direct channel and all cascaded channels can be successfully estimated and fed back to the \gls{ap}.\footnote{Due to the lack of \gls{rf} chains at the passive tag, accurate and efficient \gls{csi} acquisition at the \gls{ap} can be challenging. One possible approach is that the \gls{ap} sends pre-defined pilots, the tags respond in well-designed manners, and the user performs least-square estimation with feedbacks \cite{Bharadia2015,Yang2015b,Guo2019g}.} Since the tags need to physically switch the loads for backscatter modulation, they communicate at a much longer symbol period (and lower rates) than the primary system. As such, we assume the transitions of all tags are perfectly synchronized, and the backscatter symbol period satisfies $T_c = N T_s$ where $N \gg 1$ is a positive integer.

			Without loss of generality, we focus on the transmissions and detections during one particular backscatter symbol period. To provide a preliminary insight, we assume the primary symbol $s[n]$ at block $n \in \mathcal{N} \triangleq \{1,\ldots,N\}$ is in standard \gls{cscg} distribution, and the backscatter symbol $c_k$ of tag $k$ employs $M$-\gls{psk} modulation by \eqref{eq:mpsk}, i.e., $c_k \in \mathcal{C}$, $\forall k \in \mathcal{K}$. Thus, the signal received by the user at block $n$ can be expressed as\footnote{We omit the signal reflected by two or more times\cite{Wu2019} and assume the time difference of arrival from different paths are negligible\cite{Guo2019b}.}
			\begin{equation}
				y[n] = \left(\boldsymbol{h}_{\mathrm{D}}^H + \sum_{k \in \mathcal{K}} \sqrt{\alpha_k} \boldsymbol{h}_{\mathrm{C},k}^H c_k\right) \boldsymbol{w} s[n] + w[n],
				\label{eq:received_signal}
			\end{equation}
			where $\boldsymbol{w} \in \mathbb{C}^{Q \times 1}$ is the active precoder satisfying $\lVert \boldsymbol{w} \rVert^2 \le P$, $P$ is the average transmit power constraint at the \gls{ap}, and $w[n] \sim \mathcal{CN}(0,\sigma_w^2)$ is the equivalent \gls{awgn} at block $n$. Besides, we collect the backscatter symbols of $K$ tags into $c_{\mathcal{K}} \triangleq \{c_k : k \in \mathcal{K}\}$, stack the received signal over $N$ blocks as $\boldsymbol{y} \triangleq \left[y[1],\ldots,y[N]\right]^T \in \mathbb{C}^{N \times 1}$, and define the equivalent channel for primary transmission as
			\begin{equation}
				\boldsymbol{h}_{\mathrm{E}}^H(c_{\mathcal{K}}) \triangleq \boldsymbol{h}_{\mathrm{D}}^H + \sum_{k \in \mathcal{K}} \sqrt{\alpha_k} \boldsymbol{h}_{\mathrm{C},k}^H c_k.
				\label{eq:equivalent_primary_channel}
			\end{equation}

			\begin{remark}
				The proposed symbiotic radio system includes a multiplicative \gls{mac} where the \gls{ap} and the tags simultaneously transmit to the user. It inspired [TODO] to perform \gls{sic} that first non-coherently detects the primary message under backscatter uncertainty, then cancels its contribution and decodes the tag messages. This scheme requires non-coherent coding at the \gls{ap} and $K$ re-encoding, precoding, and subtraction operations at the user. However, different from the conventional \gls{mac} with \gls{sc}, the symbiotic radio system involves \gls{mc} that combines the primary and secondary messages by multiplication. Hence, novel multi-stream detection techniques should be tailored to symbiotic radio systems to accommodate the massive connectivity of tags and the multiplicative combination of links.
			\end{remark}
		\end{subsection}

		\begin{subsection}{Backscatter Detection}
			To reveal the impact of backscatter modulation on the primary transmission and avoid the exponential complexity of joint detection, we extend the non-coherent \gls{ambc} detection \cite{Qian2019} to the multi-tag case, and propose a low-complexity energy detection to decode the backscatter symbols under primary source uncertainty. It requires no dedicated receivers or non-coherent codes at the \gls{ap}, and can be readily implemented over legacy downlink systems.

			\begin{remark}
				One key property of symbiotic radio is the primary message propagates to the user from a known channel and multiple multiplicative channels with uncertainty introduced by backscatter modulation. As such, each reflection coefficient simultaneously encodes the tag message and influences the equivalent channel of the primary link. If the backscatter symbols can be successfully decoded first, they can be modeled within the equivalent channel \eqref{eq:equivalent_primary_channel} as in channel training, instead of being removed by \gls{sic}.
			\end{remark}

			To explicitly specify the instances of the backscatter symbols, we define the modulation index set as $m_{\mathcal{K}} \triangleq \{m_k \in \mathcal{M} : k \in \mathcal{K}\}$ and label the corresponding tag input combination as $\bar{c}_{m_{\mathcal{K}}} \triangleq \{\bar{c}_{m_k} \in \mathcal{C} : m_k \in \mathcal{M}, k \in \mathcal{K}\}$. Since any $\bar{c}_{m_{\mathcal{K}}}$ remains constant per $N$ primary symbols, the received signal at block $n$ is only subject to the variation of the primary source $s[n]$ and \gls{awgn} $w[n]$, and thus follows \gls{cscg} distribution $y_{m_{\mathcal{K}}}[n] \sim \mathcal{CN}\left(0,\sigma_{m_{\mathcal{K}}}^2\right)$, where the variance
			\begin{equation}
				\sigma_{m_{\mathcal{K}}}^2 = \Bigl\lvert \underbrace{\bigl(\boldsymbol{h}_{\mathrm{D}}^H + \sum_{k \in \mathcal{K}} \sqrt{\alpha_k} \boldsymbol{h}_{\mathrm{C},k}^H \bar{c}_{m_k}\bigr)}_{\boldsymbol{h}_{\mathrm{E}}^H(\bar{c}_{m_{\mathcal{K}}})} \boldsymbol{w} \Bigr\rvert^2 + \sigma_w^2,
				\label{eq:receive_variance}
			\end{equation}
			denotes the expectation of the received power per primary block under tag modulation index set $m_{\mathcal{K}}$. For the ease of exposition, we denote the hypothesis that the tag input combination at state $i \in \mathcal{M^K} \triangleq \{1,\ldots,M^K\}$ as $\mathcal{H}_i$, sort $\{\sigma_{m_{\mathcal{K}}}^2\}$ in ascending order by a one-to-one mapping $m_{\mathcal{K}} \mapsto i$,\footnote{When more than one modulation index sets yield the same energy level, the mapping is not unique and the detection fails to separate them. This blind spot issue can be mitigated by multi-antenna techniques.} define the received signal energy over $N$ primary blocks as $z \triangleq \lVert \boldsymbol{y} \rVert^2$, and let $f(z \mid \mathcal{H}_i)$ be the conditional probability density function of receiving $z$ under hypothesis $\mathcal{H}_i$. Correspondingly, $z$ is the sum of $N$ i.i.d. exponential variables each with mean $\sigma_i^2$, and the conditional probability density function follows Erlang distribution with shape $N$ and scale $\sigma_i^2$ as
			\begin{equation}
				f(z \mid \mathcal{H}_i) = \frac{z^{N-1} e^{-z/\sigma_i^2}}{\sigma_i^{2N} (N-1)!}.
				\label{eq:energy_distribution}
			\end{equation}

			\begin{remark}
				The backscatter links essentially form a discrete-input continuous-output memoryless channel. To further reduce decoding complexity, we apply hard-decision detection and construct a \gls{dmtc}, whose capacity is a function of both input distribution and decision thresholds \cite{Nguyen2018}.
			\end{remark}

			Denote the decision region of hypothesis $\mathcal{H}_i$ as
			\begin{equation}
				\mathcal{R}_i \triangleq [T_{i-1,i}, T_{i,i+1}),
				\label{eq:decision_region}
			\end{equation}
			where $T_{i-1,i}$ is the decision threshold between $\mathcal{H}_{i-1}$ and $\mathcal{H}_i$, and $T_{i,i+1}$ is the decision threshold between $\mathcal{H}_i$ and $\mathcal{H}_{i+1}$. We also define $T_{0,1} \triangleq 0$, $T_{M^K,M^K+1} \triangleq \infty$, and $\boldsymbol{t} \triangleq [T_{0,1},\ldots,T_{M^K,M^K+1}]^T \in \mathbb{R}^{(M^K + 1) \times 1}$.

			Consider the (\gls{ml}) detector for example. The likelihood ratio between hypotheses $\mathcal{H}_i$ and $\mathcal{H}_{i'}$ is \cite{Qian2019}
			\begin{equation}
				\Lambda_{i,{i'}}^{\mathrm{\gls{ml}}}(z) = \frac{f(z \mid \mathcal{H}_i)}{f(z \mid \mathcal{H}_{i'})} = \left( \frac{\sigma_{i'}^2}{\sigma_i^2} \right)^N \exp \left( \frac{\sigma_i^2 - \sigma_{i'}^2}{\sigma_i^2 \sigma_{i'}^2} z \right),
				\label{eq:likelihood_ratio}
			\end{equation}
			the corresponding decision rule is
			\begin{equation}
				\Lambda_{i,{i'}}^{\mathrm{\gls{ml}}}(z) \underset{\mathcal{H}_{i'}}{\overset{\mathcal{H}_i}{\lessgtr}} 1 \iff z \underset{\mathcal{H}_{i'}}{\overset{\mathcal{H}_i}{\lessgtr}} T_{i,{i'}}^{\mathrm{\gls{ml}}},
				\label{eq:decision_rule}
			\end{equation}
			and the detection threshold is
			\begin{equation}
				T_{i,{i'}}^{\mathrm{\gls{ml}}} \triangleq N \frac{\sigma_i^2 \sigma_{i'}^2}{\sigma_i^2 - \sigma_{i'}^2} \log \frac{\sigma_i^2}{\sigma_{i'}^2}.
				\label{eq:detection_threshold}
			\end{equation}

			\begin{remark}
				Although the \gls{ml} decision threshold is optimal in terms of the likelihood function, it is not necessarily capacity-achieving, although the performance gap can be negligible in the single-tag \gls{bibo} case.
			\end{remark}

			Once the decision region is determined, we can formulate an equivalent point-to-point \gls{dmc} from tag input combination alphabet $\mathcal{M^K}$ to output energy level alphabet $\mathcal{M^K}$. The probability of receiving energy level $j$ under tag input combination $i$ is\footnote{For simplicity, we assume there exists at least one feasible precoder that produces distinct received energy levels for all tag input combinations. [TODO] Merge with precoder design.}
			\begin{equation}
				P(\bar{z}_j \mid \bar{c}_i) = P(z \in \mathcal{R}_j \mid \mathcal{H}_i) = \int_{\mathcal{R}_j} f(z \mid \mathcal{H}_i) \dd z.
				\label{eq:point_to_point_channel}
			\end{equation}

			On top of this, we can compute the marginal probability of each tag and obtain $K$ transition matrices from $\mathcal{M}$ to $\mathcal{M^K}$ that compose a discrete memoryless \gls{mac}. The probability of observing energy level $j$ when tag $k$ at state $m_k$ is
			\begin{equation}
				P(\bar{z}_j \mid \bar{c}_{m_k}) = \frac{\sum_{m_{\mathcal{K} \setminus \{k\}}} P(\bar{z}_j \mid \bar{c}_{m_{\mathcal{K}}})}{\sum_{m_{\mathcal{K}}} P(\bar{z}_j \mid \bar{c}_{m_{\mathcal{K}}})}.
				\label{eq:mac}
			\end{equation}

			In summary, with the \gls{csi} knowledge and for any given precoder and detection threshold set, we can obtain the expected power of the received signal per block by \eqref{eq:receive_variance}, the conditional probability density function of the accumulated energy by \eqref{eq:energy_distribution}, the decision region by \eqref{eq:decision_region}, the equivalent point-to-point channel by \eqref{eq:point_to_point_channel}, and the discrete memoryless \gls{mac} by \eqref{eq:mac}.
		\end{subsection}

		\begin{subsection}{Sum backscatter rate}
			We first introduce some prerequisites of information theory. Define the input probability distribution of tag $k$ at state $m_k$ as $P_k(\bar{c}_{m_k})$, and let $\boldsymbol{p}_k \triangleq [P_k(\bar{c}_1),\ldots,P_k(\bar{c}_M)]^T \in \mathbb{R}^{M \times 1}$, $\boldsymbol{P} \triangleq [\boldsymbol{p}_1^T,\ldots,\boldsymbol{p}_K^T]^T \in \mathbb{R}^{M \times K}$. Assume the input distribution of all tags are mutually independent such that the probability associated with input combination $\bar{c}_{m_{\mathcal{K}}}$ satisfies $P_{\mathcal{K}}(\bar{c}_{m_{\mathcal{K}}}) = \prod_{k \in \mathcal{K}} P_k(\bar{c}_{m_k})$. Following \cite{Rezaeian2004}, the corresponding backscatter information function is defined as
			\begin{equation}
				I_{\mathrm{B}}(\bar{c}_{m_{\mathcal{K}}};z) \triangleq \sum_j P(\bar{z}_j \mid \bar{c}_{m_{\mathcal{K}}}) \log \frac{P(\bar{z}_j \mid \bar{c}_{m_{\mathcal{K}}})}{P(\bar{z}_j)},
				\label{eq:backscatter_information_function}
			\end{equation}
			where $P(\bar{z}_j) = \sum_{m_{\mathcal{K}}} P_{\mathcal{K}}(\bar{c}_{m_{\mathcal{K}}}) P(\bar{z}_j \mid \bar{c}_{m_{\mathcal{K}}})$.
			The backscatter marginal information function of tag $k \in \mathcal{K}$ associated with $\bar{c}_{m_k}$, $m_k \in \mathcal{M}$ is defined as
			\begin{equation}
				I_{\mathrm{B},k}(\bar{c}_{m_k};z) \triangleq \sum_{m_{\mathcal{K} \setminus \{k\}}} P_{\mathcal{K} \setminus \{k\}}(\bar{c}_{m_{\mathcal{K} \setminus \{k\}}}) I_{\mathrm{B}}(\bar{c}_{m_{\mathcal{K} \setminus \{k\}}},\bar{c}_{m_k};z),
				\label{eq:backscatter_marginal_information_function}
			\end{equation}
			where $P_{\mathcal{K} \setminus \{k\}}(\bar{c}_{m_{\mathcal{K} \setminus \{k\}}}) = \prod_{q \in \mathcal{K} \setminus \{k\}} P_{q}(\bar{c}_{m_{q}})$. The backscatter mutual information can be expressed as\footnote{Please be aware that $c_{\mathcal{K}}$, $c_k$, $z$ are random variables, while $\bar{c}_{m_{\mathcal{K}}}$ and $\bar{c}_i$, $\bar{c}_{m_k}$, $\bar{z}_j$ represent the corresponding instances.}
			\begin{subequations}
				\begin{align}
					I_{\mathrm{B}}(c_{\mathcal{K}};z)
					& = \mathbb{E}_{c_{\mathcal{K}}} \left[I_{\mathrm{B}}(\bar{c}_{m_{\mathcal{K}}};z)\right] = \mathbb{E}_{c_k} \left[I_{\mathrm{B},k}(\bar{c}_{m_k};z)\right]\label{eq:backscatter_sum_rate_expectation}\\
					& = \sum_{m_{\mathcal{K}}} P_{\mathcal{K}}(\bar{c}_{m_{\mathcal{K}}}) \sum_j P(\bar{z}_j \mid \bar{c}_{m_{\mathcal{K}}}) \log \frac{P(\bar{z}_j \mid \bar{c}_{m_{\mathcal{K}}})}{P(\bar{z}_j)}.\label{eq:backscatter_sum_rate_expansion}
				\end{align}
				\label{eq:backscatter_sum_rate}
			\end{subequations}
			Evidently, \eqref{eq:backscatter_information_function}--\eqref{eq:backscatter_sum_rate} are functions of the tag input distribution $\boldsymbol{P}$ as well as the discrete memoryless \gls{mac} $P(z \mid c_{\mathcal{K}})$, and thus depend on the transmit precoder $\boldsymbol{w}$ and the detection threshold $\boldsymbol{t}$.
		\end{subsection}

		\begin{subsection}{Ergodic primary rate}
			Once the tag input combination is successfully decoded, the user can eliminate backscatter uncertainty and model its contribution within the equivalent primary channel \eqref{eq:equivalent_primary_channel}. As such, the tags can adjust the propagation environment in a potentially beneficial manner, and create artificial channel variation within each fading block. Similarly, we define the primary information function associated with tag input combination $\bar{c}_{m_{\mathcal{K}}}$ and the primary marginal information function of tag $k$ associated with symbol $\bar{c}_{m_k}$ respectively as

			\begin{align}
				I_{\mathrm{P}}(\bar{c}_{m_{\mathcal{K}}};\boldsymbol{y})
				& \triangleq N \log_2 \left(1 + \frac{\lvert \boldsymbol{h}_{\mathrm{E}}^H(\bar{c}_{m_{\mathcal{K}}}) \boldsymbol{w} \rvert^2}{\sigma_w^2}\right),\label{eq:primary_information_function}\\
				I_{\mathrm{P},k}(\bar{c}_{m_k};\boldsymbol{y})
				& \triangleq \sum_{m_{\mathcal{K} \setminus \{k\}}} P_{\mathcal{K} \setminus \{k\}}(\bar{c}_{m_{\mathcal{K} \setminus \{k\}}}) I_{\mathrm{P}}(\bar{c}_{m_{\mathcal{K} \setminus \{k\}}},\bar{c}_{m_k};\boldsymbol{y}),\label{eq:primary_marginal_information_function}
			\end{align}
			and the ergodic primary rate can be expressed as\footnote{The unit of \eqref{eq:backscatter_information_function}--\eqref{eq:backscatter_sum_rate} are \gls{bpcu}, while the unit of \eqref{eq:primary_information_function}--\eqref{eq:primary_ergodic_rate} are \gls{bpsphz}.}
			\begin{subequations}
				\begin{align}
					I_{\mathrm{P}}(c_{\mathcal{K}};\boldsymbol{y})
					& = \mathbb{E}_{c_{\mathcal{K}}} \left[I_{\mathrm{P}}(\bar{c}_{m_{\mathcal{K}}};\boldsymbol{y})\right] = \mathbb{E}_{c_k} \left[I_{\mathrm{P},k}(\bar{c}_{m_k};\boldsymbol{y})\right]\label{eq:primary_ergodic_rate_expectation}\\
					& = \sum_{m_{\mathcal{K}}} P_{\mathcal{K}}(\bar{c}_{m_{\mathcal{K}}}) N \log_2 \left(1 + \frac{\lvert \boldsymbol{h}_{\mathrm{E}}^H(\bar{c}_{m_{\mathcal{K}}}) \boldsymbol{w} \rvert^2}{\sigma_w^2}\right).
					\label{eq:primary_ergodic_rate_expansion}
				\end{align}
				\label{eq:primary_ergodic_rate}
			\end{subequations}

			In contrast to the backscatter case, \eqref{eq:primary_information_function}--\eqref{eq:primary_ergodic_rate} are irrelevant to the detection threshold $\boldsymbol{t}$, but depend on the tag input probability distribution $\boldsymbol{P}$ and the transmit precoder $\boldsymbol{w}$.
		\end{subsection}
	\end{section}

	\begin{section}{Input Distribution, Decision Region, and Precoder Design}
		\begin{subsection}{Primary-backscatter rate region}
			Define the weighted sum information function associated with tag input combination $\bar{c}_{m_{\mathcal{K}}}$ as
			\begin{equation}
				I(\bar{c}_{m_{\mathcal{K}}};\boldsymbol{y},z) \triangleq \rho I_{\mathrm{P}}(\bar{c}_{m_{\mathcal{K}}};\boldsymbol{y}) + (1 - \rho) I_{\mathrm{B}}(\bar{c}_{m_{\mathcal{K}}};z),
				\label{eq:weighted_sum_information_function}
			\end{equation}
			the weighted sum marginal information of tag $k$ associated with symbol $\bar{c}_{m_k}$ as
			\begin{equation}
				I_k(\bar{c}_{m_k};\boldsymbol{y},z) \triangleq \rho I_{\mathrm{P},k}(\bar{c}_{m_k};\boldsymbol{y}) + (1 - \rho) I_{\mathrm{B},k}(\bar{c}_{m_k};z),
				\label{eq:weighted_sum_marginal_information}
			\end{equation}
			and the weighted sum primary-backscatter rate as
			\begin{equation}
				I(c_{\mathcal{K}};\boldsymbol{y},z) \triangleq \rho I_{\mathrm{P}}(c_{\mathcal{K}};\boldsymbol{y}) + (1 - \rho) I_{\mathrm{B}}(c_{\mathcal{K}};z),
				\label{eq:weighted_sum_rate}
			\end{equation}
			where $\rho \in [0,1]$ denotes the priority of the primary link. To investigate how backscatter modulation and detection may influence the primary transmission, we optimize the input probability distribution, decision region, and transmit precoder to maximize the weighted sum primary-backscatter rate
			\begin{maxi!}
				{\scriptstyle{\boldsymbol{P},\boldsymbol{t},\boldsymbol{w}}}{I(c_{\mathcal{K}};\boldsymbol{y},z)}{\label{op:rate_region}}{\label{ob:weighted_sum_rate}}
				\addConstraint{\sum \nolimits_{m_k} P_k(\bar{c}_{m_k})}{=1,\quad\label{co:sum_probability}}{\forall k \in \mathcal{K}}
				\addConstraint{P_k(\bar{c}_{m_k})}{\ge 0,\quad\label{co:nonnegative_probability}}{\forall m_k \in \mathcal{M}, \ \forall k \in \mathcal{K}}
				\addConstraint{\lVert \boldsymbol{w} \rVert^2}{\le P.\label{co:average_transmit_power}}
			\end{maxi!}
			where \eqref{co:sum_probability} and \eqref{co:nonnegative_probability} are input probability constraints and \eqref{co:average_transmit_power} is the average transmit power budget. As problem \eqref{op:rate_region} is not jointly convex over $\boldsymbol{P}$, $\boldsymbol{t}$ and $\boldsymbol{w}$, we propose a \gls{bcd} method that iteratively updates the input distribution, decision region, and transmit precoder, until convergence is achieved.
		\end{subsection}

		\begin{subsection}{Input probability distribution}
			For any fixed decision boundary $\boldsymbol{t}$ and transmit precoder $\boldsymbol{w}$, the equivalent discrete memoryless \gls{mac} can be determined by \eqref{eq:mac} and problem~\eqref{op:rate_region} boils down to
			\begin{maxi!}
				{\scriptstyle{\boldsymbol{P}}}{I(c_{\mathcal{K}};\boldsymbol{y},z)}{\label{op:input_probability_distribution}}{}
				\addConstraint{\eqref{co:sum_probability},\eqref{co:nonnegative_probability},}
			\end{maxi!}
			which is non-convex for $K > 1$ due to the product term $\prod_{k \in \mathcal{K}} P_k(\bar{c}_{m_k})$. Fortunately, the \gls{kkt} conditions are necessary and sufficient for this type of problem, and the proof follows straightforwardly from \cite{Watanabe2009}.\footnote{For any elementary discrete memoryless \gls{mac} (sizes of input alphabets are no greater than that of output alphabet), the sufficiency can be proved by combining the local maximum and connectedness of \gls{kkt} solutions. For any general discrete memoryless \gls{mac}, the capacity can be achieved by an elementary discrete memoryless \gls{mac} included within. Thus, we restrict the discussion of this paper to elementary discrete memoryless \gls{mac}s.}
			\begin{proposition}
				The necessary and sufficient conditions for an input probability distribution $\boldsymbol{P}^{\star}$ to maximize the weighted sum primary-backscatter rate are that, $\forall m_k \in \mathcal{M}$ and $\forall k \in \mathcal{K}$,
				\begin{subequations}
					\label{eq:optimal_conditions}
					\begin{alignat}{2}
						I_k^{\star}(\bar{c}_{m_k};\boldsymbol{y},z) & = I^{\star}(c_{\mathcal{K}};\boldsymbol{y},z), \quad && P_k^{\star}(\bar{c}_{m_k}) > 0,\label{eq:probable_states}\\
						I_k^{\star}(\bar{c}_{m_k};\boldsymbol{y},z) & \le I^{\star}(c_{\mathcal{K}};\boldsymbol{y},z), \quad && P_k^{\star}(\bar{c}_{m_k}) = 0.\label{eq:dropped_states}
					\end{alignat}
				\end{subequations}
				\eqref{eq:probable_states} means each probable state of each tag should produce the same marginal information (averaged over all states of other tags), while \eqref{eq:dropped_states} implies any state of any tag with potentially less marginal information than above should not be used.
				\label{pr:optimal_conditions}
			\end{proposition}
			\begin{proof}
				Denote the Lagrange multiplier associated with \eqref{co:sum_probability} and \eqref{co:nonnegative_probability} as $\boldsymbol{\mu} \triangleq [\mu_1,\ldots,\mu_K]^T \in \mathbb{R}^{K \times 1}$ and $\boldsymbol{\Lambda} \triangleq [\boldsymbol{\lambda}_1^T,\ldots,\boldsymbol{\lambda}_K^T]^T \in \mathbb{R}^{M \times K}$ with $\boldsymbol{\lambda}_k \triangleq [\lambda_{k,1},\ldots,\lambda_{k,M}]^T \in \mathbb{R}^{M \times 1}$, respectively. The Lagrangian function of problem~\eqref{op:input_probability_distribution} is
				\begin{align}
					L(\boldsymbol{P},\boldsymbol{\mu},\boldsymbol{\Lambda})
					& = - I(c_{\mathcal{K}};\boldsymbol{y},z) + \sum_{k \in \mathcal{K}} \mu_k \left( \sum_{m_k \in \mathcal{M}} P_k(\bar{c}_{m_k}) - 1 \right)\nonumber\\
					& \quad - \sum_{k \in \mathcal{K}} \sum_{m_k \in \mathcal{M}} \lambda_{k,m_k} P_k(\bar{c}_{m_k}),
				\end{align}
				and the corresponding \gls{kkt} conditions on the primal and dual solutions are, $\forall m_k \in \mathcal{M}$ and $\forall k \in \mathcal{K}$,
				\begin{subequations}
					\begin{equation}
						- \nabla_{P_k^{\star}(\bar{c}_{m_k})} I^{\star}(c_{\mathcal{K}};\boldsymbol{y},z) + \mu_k^{\star} - \lambda_{k,m_k}^{\star} = 0,
					\end{equation}
					\begin{equation}
						\lambda_{k,m_k}^{\star} = 0, \quad P_k^{\star}(\bar{c}_{m_k}) > 0,
					\end{equation}
					\begin{equation}
						\lambda_{k,m_k}^{\star} \ge 0, \quad P_k^{\star}(\bar{c}_{m_k}) = 0,
					\end{equation}
				\end{subequations}
				where the directional derivative can be explicitly expressed as
				\begin{equation}
					\nabla_{P_k^{\star}(\bar{c}_{m_k})} I^{\star}(c_{\mathcal{K}};\boldsymbol{y},z) = I_k^{\star}(\bar{c}_{m_k};\boldsymbol{y},z) - (1 - \rho).
				\end{equation}

				Due to the necessity and sufficiency of the \gls{kkt} conditions, we conclude that any input probability distribution $\boldsymbol{P}^{\star}$ maximizes the weighted sum primary-backscatter rate if and only if, $\forall m_k \in \mathcal{M}$ and $\forall k \in \mathcal{K}$,
				\begin{subequations}
					\begin{alignat}{2}
						I_k^{\star}(\bar{c}_{m_k};\boldsymbol{y},z) & = \mu_k^{\star} + (1 - \rho), \quad && P_k^{\star}(\bar{c}_{m_k}) > 0,\label{eq:probable_states_marginal}\\
						I_k^{\star}(\bar{c}_{m_k};\boldsymbol{y},z) & \le \mu_k^{\star} + (1 - \rho), \quad && P_k^{\star}(\bar{c}_{m_k}) = 0,\label{eq:dropped_states_marginal}
					\end{alignat}
				\end{subequations}
				which implies
				\begin{equation}
					\sum_{m_k} P_k^{\star}(\bar{c}_{m_k}) I_k^{\star}(\bar{c}_{m_k};\boldsymbol{y},z) = \mu_k^{\star} + (1 - \rho).
				\end{equation}

				On the other hand, by definition of marginations \eqref{eq:backscatter_sum_rate_expectation}, \eqref{eq:primary_ergodic_rate_expectation} and weighted summations \eqref{eq:weighted_sum_marginal_information}, \eqref{eq:weighted_sum_rate}, we have
				\begin{equation}
					\sum_{m_k} P_k^{\star}(\bar{c}_{m_k}) I_k^{\star}(\bar{c}_{m_k};\boldsymbol{y},z) = I^{\star}(c_{\mathcal{K}};\boldsymbol{y},z),
				\end{equation}
				which is irrelevant to $k$. It suggests $I^{\star}(c_{\mathcal{K}};\boldsymbol{y},z) = \mu_k^{\star} + (1 - \rho)$, $\forall k \in \mathcal{K}$, and completes the proof.
			\end{proof}

			Next, we extend the Blahut-Arimoto algorithm to the proposed case and obtain the optimal input probability distribution as follows.

			\begin{theorem}
				The input probability distribution that achieves the weighted sum primary-backscatter capacity, at state $m_k \in \mathcal{M}$ of tag $k \in \mathcal{K}$, is given by the converging point of sequence\footnote{Note that the probability distribution of tag $k$ is based on the updated probability distribution of tags $1,\ldots,k-1$.}
				\begin{equation}
					P_k^{(r+1)}(\bar{c}_{m_k}) = \frac{P_k^{(r)}(\bar{c}_{m_k}) \exp \left( \frac{\rho}{1 - \rho} I_k^{(r)}(\bar{c}_{m_k};\boldsymbol{y},z) \right)}{\sum_{m_k'} P_k^{(r)}(\bar{c}_{m_k'}) \exp \left( \frac{\rho}{1 - \rho} I_k^{(r)}(\bar{c}_{m_k'};\boldsymbol{y},z) \right)},
					\label{eq:optimal_distribution}
				\end{equation}
				where $r \in \mathbb{Z}_+$ is the iteration index and $P_k^{(0)}(\bar{c}_{m_k}) > 0$.
				% , $\forall m_k \in \mathcal{M}$ and $\forall k \in \mathcal{K}$.
				\label{th:optimal_distribution}
			\end{theorem}
			\begin{proof}
				We first prove sequence~\eqref{eq:optimal_distribution} is non-decreasing in mutual information. For state $m_k \in \mathcal{M}$ of tag $k \in \mathcal{K}$, let $P_{\mathcal{K}}(\bar{c}_{m_{\mathcal{K}}}) = \prod_{q \in \mathcal{K}} P_q(\bar{c}_{m_q})$ and $P_{\mathcal{K}}'(\bar{c}_{m_{\mathcal{K}}}) = P_k'(\bar{c}_{m_k}) \prod_{q \in \mathcal{K} \setminus \{k\}} P_q(\bar{c}_{m_q})$ be two probability distributions with potentially different marginal for tag $k$, and define an intermediate function $J \left( P_{\mathcal{K}}(\bar{c}_{m_{\mathcal{K}}}),P_{\mathcal{K}}'(\bar{c}_{m_{\mathcal{K}}}) \right)$ by \eqref{eq:intermediate_information_function}. Apparently, $I(\bar{c}_{\mathcal{K}};\boldsymbol{y},z) = J \left( P_{\mathcal{K}}(\bar{c}_{m_{\mathcal{K}}}),P_{\mathcal{K}}(\bar{c}_{m_{\mathcal{K}}}) \right)$.
				\begin{figure*}[!b]
					\hrule
					\begin{align}
						J \left( P_{\mathcal{K}}(\bar{c}_{m_{\mathcal{K}}}),P_{\mathcal{K}}'(\bar{c}_{m_{\mathcal{K}}}) \right)
						& \triangleq \rho \sum_{m_{\mathcal{K}}} P_{\mathcal{K}}(\bar{c}_{m_{\mathcal{K}}}) N \log_2 \left(1 + \frac{\lvert \boldsymbol{h}_{\mathrm{E}}^H(\bar{c}_{m_{\mathcal{K}}}) \boldsymbol{w} \rvert^2}{\sigma_w^2}\right)\nonumber\\
						& \quad + (1 - \rho) \sum_{m_{\mathcal{K}}} P_{\mathcal{K}}(\bar{c}_{m_{\mathcal{K}}}) \sum_j P(\bar{z}_j \mid \bar{c}_{m_{\mathcal{K}}}) \log \frac{P(\bar{z}_j \mid \bar{c}_{m_{\mathcal{K}}}) P_{\mathcal{K}}'(\bar{c}_{m_{\mathcal{K}}})}{P'(\bar{z}_j) P_{\mathcal{K}}(\bar{c}_{m_{\mathcal{K}}})}.
						\label{eq:intermediate_information_function}
					\end{align}
				\end{figure*}
				For a fixed $P_{\mathcal{K}}'(\bar{c}_{m_{\mathcal{K}}})$, $J \left( P_{\mathcal{K}}(\bar{c}_{m_{\mathcal{K}}}),P_{\mathcal{K}}'(\bar{c}_{m_{\mathcal{K}}}) \right)$ is a concave function of $P_k(\bar{c}_{m_k})$ and is maximized at $\nabla_{P_k^{\star}(\bar{c}_{m_k})} J \left( P_{\mathcal{K}}(\bar{c}_{m_{\mathcal{K}}}),P_{\mathcal{K}}'(\bar{c}_{m_{\mathcal{K}}}) \right) = 0$, namely
				\begin{equation}
					S_k'(\bar{c}_{m_k}) - S_k'(\bar{c}_{i_k}) + (1 - \rho) \log \frac{P_k(\bar{c}_{i_k})}{P_k^{\star}(\bar{c}_{m_k})} = 0,
					\label{eq:optimal_intermediate_information_condition}
				\end{equation}
				where $i_k \ne m_k$ is the reference state and
				\begin{align}
					S_k'(\bar{c}_{m_k})
					& \triangleq I_k'(\bar{c}_{m_k};\boldsymbol{y},z) + (1 - \rho) \sum_{m_{\mathcal{K} \setminus \{k\}}} P_{\mathcal{K} \setminus \{k\}}(\bar{c}_{m_{\mathcal{K} \setminus \{k\}}})\nonumber\\
					& \quad \times \sum_j P(\bar{z}_j \mid \bar{c}_{m_{\mathcal{K} \setminus \{k\}}},\bar{c}_{m_k}) \log P_{\mathcal{K}}'(\bar{c}_{m_{\mathcal{K} \setminus \{k\}}},\bar{c}_{m_k}).
				\end{align}
				Evidently, $\forall m_k \ne i_k$, \eqref{eq:optimal_intermediate_information_condition} boils down to
				% \begin{subequations}
				% 	\begin{align}
				% 		P_k^{\star}(\bar{c}_{m_k})
				% 		& = \frac{\exp \left( \frac{1}{1 - \rho} S_k'(\bar{c}_{m_k}) \right)}{\sum_{m_k'} \exp \left( \frac{1}{1 - \rho} S_k'(\bar{c}_{m_k'}) \right)}\\
				% 		& = \frac{P_k'(\bar{c}_{m_k}) \exp \left( \frac{\rho}{1 - \rho} I_k'(\bar{c}_{m_k};\boldsymbol{y},z) \right)}{\sum_{m_k'} P_k'(\bar{c}_{m_k'}) \exp \left( \frac{\rho}{1 - \rho} I_k'(\bar{c}_{m_k'};\boldsymbol{y},z) \right)}.
				% 		\label{eq:optimal_relative_distribution}
				% 	\end{align}
				% \end{subequations}
				\begin{subequations}
					\begin{align}
						P_k^{\star}(\bar{c}_{m_k})
						& = \frac{\exp \left( \frac{1}{1 - \rho} S_k'(\bar{c}_{m_k}) \right)}{\sum_{m_k'} \exp \left( \frac{1}{1 - \rho} S_k'(\bar{c}_{m_k'}) \right)}\\
						& = \frac{P_k'(\bar{c}_{m_k}) u_k'(\bar{c}_{m_k})}{\sum_{m_k'} P_k'(\bar{c}_{m_k'}) u_k'(\bar{c}_{m_k'})},
						\label{eq:optimal_relative_distribution}
					\end{align}
				\end{subequations}
				where we define $u_k'(\bar{c}_{m_k}) \triangleq \exp \left( \frac{\rho}{1 - \rho} I_k'(\bar{c}_{m_k};\boldsymbol{y},z) \right)$. Although $P_k(\bar{c}_{i_k}) = 1 - \sum_{m_k \ne i_k} P_k^{\star}(\bar{c}_{m_k})$ is not guaranteed to be optimal, it has exactly the same form as \eqref{eq:optimal_relative_distribution}. In such case, the solution of choosing $i_k$ as reference would coincide with that of choosing any $m_k \ne i_k$ as reference, while optimality for $i_k$ is guaranteed in the latter case. Therefore, for a fixed $P_{\mathcal{K}}'(\bar{c}_{m_{\mathcal{K}}})$, $\forall i_k \in \mathcal{M}$ and $\forall k \in \mathcal{K}$, \eqref{eq:optimal_relative_distribution} ensures
				\begin{equation}
					J \left( P_{\mathcal{K}}(\bar{c}_{m_{\mathcal{K}}}),P_{\mathcal{K}}'(\bar{c}_{m_{\mathcal{K}}}) \right) \ge I'(\bar{c}_{\mathcal{K}};\boldsymbol{y},z).
					\label{eq:information_difference_lower}
				\end{equation}

				On the other hand, choosing $P_k(\bar{c}_{m_k})$ by \eqref{eq:optimal_relative_distribution} also satisfies
				\begin{subequations}
					\label{eq:information_difference_upper}
					\begin{align}
						\Delta
						& \triangleq I(\bar{c}_{\mathcal{K}};\boldsymbol{y},z) - J \left( P_{\mathcal{K}}(\bar{c}_{m_{\mathcal{K}}}),P_{\mathcal{K}}'(\bar{c}_{m_{\mathcal{K}}}) \right)\\
						& = (1 - \rho) \sum_{m_{\mathcal{K}}} P_{\mathcal{K}}(\bar{c}_{m_{\mathcal{K}}}) \sum_j P(\bar{z}_j \mid \bar{c}_{m_{\mathcal{K}}}) \log \frac{P'(\bar{z}_j) P_k(\bar{c}_{m_k})}{P(\bar{z}_j) P_k'(\bar{c}_{m_k})}\\
						& = (1 - \rho) \sum_{m_k} \frac{P_k'(\bar{c}_{m_k}) u_k'(\bar{c}_{m_k})}{\sum_{m_k'} P_k'(\bar{c}_{m_k'}) u_k'(\bar{c}_{m_k'})} \sum_j P(\bar{z}_j \mid \bar{c}_{m_k})\nonumber\\
						& \quad \times \log \frac{\sum_{m_k'} P_k'(\bar{c}_{m_k'}) P(\bar{z}_j \mid \bar{c}_{m_k'}) u_k'(\bar{c}_{m_k})}{\sum_{m_k'} P_k'(\bar{c}_{m_k'}) P(\bar{z}_j \mid \bar{c}_{m_k'}) u_k'(\bar{c}_{m_k'})}\\
						& \ge (1 - \rho) \sum_{m_k} \frac{P_k'(\bar{c}_{m_k}) u_k'(\bar{c}_{m_k})}{\sum_{m_k'} P_k'(\bar{c}_{m_k'}) u_k'(\bar{c}_{m_k'})} \sum_j P(\bar{z}_j \mid \bar{c}_{m_k})\nonumber\\
						& \quad \times \left( 1 - \frac{\sum_{m_k'} P_k'(\bar{c}_{m_k'}) P(\bar{z}_j \mid \bar{c}_{m_k'}) u_k'(\bar{c}_{m_k'})}{\sum_{m_k'} P_k'(\bar{c}_{m_k'}) P(\bar{z}_j \mid \bar{c}_{m_k'}) u_k'(\bar{c}_{m_k})} \right)\\
						& = 0,
					\end{align}
				\end{subequations}
				where the equality holds if and only if \eqref{eq:optimal_relative_distribution} converges. Combining \eqref{eq:information_difference_lower} and \eqref{eq:information_difference_upper} suggests $I(\bar{c}_{\mathcal{K}};\boldsymbol{y},z) \ge I'(\bar{c}_{\mathcal{K}};\boldsymbol{y},z)$. Since mutual information is bounded above, we conclude sequence \eqref{eq:optimal_distribution} is non-decreasing and convergent in mutual information.

				We then prove any converging point of sequence \eqref{eq:optimal_distribution}, denoted as $P_k^{\star}(\bar{c}_{m_k})$, fulfills the optimal conditions \eqref{eq:optimal_conditions} and achieves the weighted primary-backscatter sum capacity. To see this, define
				\begin{equation}
					D_k^{(r)}(\bar{c}_{m_k}) \triangleq \frac{P_k^{(r+1)}(\bar{c}_{m_k})}{P_k^{(r)}(\bar{c}_{m_k})} = \frac{u_k^{(r)}(\bar{c}_{m_k})}{\sum_{m_k'} P_k^{(r)}(\bar{c}_{m_k'}) u_k^{(r)}(\bar{c}_{m_k'})},
				\end{equation}
				where $P_k^{(0)}(\bar{c}_{m_k}) > 0$. As sequence \eqref{eq:optimal_distribution} is convergent, any state with $P_k^{\star}(\bar{c}_{m_k}) > 0$ need to satisfy $D_k^{\star}(\bar{c}_{m_k}) \triangleq \lim_{r \to \infty} D_k^{(r)}(\bar{c}_{m_k}) = 1$, namely
				\begin{equation}
					I_k^{\star}(\bar{c}_{m_k};\boldsymbol{y},z) = \frac{1 - \rho}{\rho} \log \sum_{m_k'} P_k^{\star}(\bar{c}_{m_k'}) u_k^{\star}(\bar{c}_{m_k'}),
				\end{equation}
				which coincides with \eqref{eq:probable_states_marginal} and implies \eqref{eq:probable_states}. That is, given $P_k^{(0)}(\bar{c}_{m_k}) > 0$, any converging point with $P_k^{\star}(\bar{c}_{m_k}) > 0$ must satisfy \eqref{eq:probable_states}.
				On the other hand, we assume $P_k^{\star}(\bar{c}_{m_k})$ does not satisfy \eqref{eq:dropped_states}, such that for any state with $P_k^{\star}(\bar{c}_{m_k}) = 0$,
				% On the other hand, we show if $P_k^{\star}(\bar{c}_{m_k})$ does not satisfy \eqref{eq:dropped_states}, then it is not a converging point of sequence \eqref{eq:optimal_distribution}. By this assumption, for any state with $P_k^{\star}(\bar{c}_{m_k}) = 0$,
				\begin{equation}
					I_k^{\star}(\bar{c}_{m_k};\boldsymbol{y},z) > I^{\star}(c_{\mathcal{K}};\boldsymbol{y},z) = \sum_{m_k'} P_k^{\star}(\bar{c}_{m_k'}) I_k^{\star}(\bar{c}_{m_k'};\boldsymbol{y},z),
				\end{equation}
				where the equality origins from \eqref{eq:backscatter_sum_rate_expectation} and \eqref{eq:primary_ergodic_rate_expectation}. Since the exponential function is monotonically increasing, we have $u_k^{\star}(\bar{c}_{m_k}) > \sum_{m_k'} P_k^{\star}(\bar{c}_{m_k'}) u_k^{\star}(\bar{c}_{m_k'})$ and $D_k^{\star}(\bar{c}_{m_k}) > 1$. Considering $P_k^{(0)}(\bar{c}_{m_k}) > 0$ and $P_k^{\star}(\bar{c}_{m_k}) = 0$, it contradicts with
				\begin{equation}
					P_k^{(r)}(\bar{c}_{m_k}) = P_k^{(0)}(\bar{c}_{m_k}) \prod_{n=1}^r D_k^{(n)}(\bar{c}_{m_k}).
				\end{equation}
				Therefore, given $P_k^{(0)}(\bar{c}_{m_k}) > 0$, any converging point with $P_k^{\star}(\bar{c}_{m_k}) = 0$ must satisfy \eqref{eq:dropped_states}.

				In conclusion, sequence \eqref{eq:optimal_distribution} always converges to a weighted sum capacity-achieving distribution for state $m_k$ of tag $k$.
			\end{proof}
		\end{subsection}


		% \begin{subsection}{Decision region}
		% 	As indicated by \cite{Qian2019b}, the optimal \gls{ml} decision regions are very close to the optimal decision regions to problem \eqref{op:rate_region}. We can either use \eqref{eq:decision_region} as suboptimal, or take derivative of \eqref{eq:backscatter_sum_rate} w.r.t. $T_{i-1,i}$ and $T_{i,i+1}$ (however, closed-form solutions are unavailable and two-dimensional search is needed).
		% \end{subsection}

		% \begin{subsection}{Precoder}
		% 	Interestingly, we can design precoder to adjust the expectation of the received power \eqref{eq:receive_variance} at each tag input combination, which can avoid the detection blind spots in \cite{Qian2019} and further boost the weighted sum-rate. However, the problem is highly non-convex -- the information function associated with input combination state $i$ is
		% 	\begin{align}
		% 		I(\bar{c}_i;z)
		% 		& = \sum_{j \in \mathcal{M^K}} \int_{T_{j-1,j}}^{T_{j,j+1}} \frac{z^{N-1} \exp \left(-\frac{z}{\mathrm{tr}(H_{\mathrm{E},i} W) + \sigma_w^2}\right)}{\left(\mathrm{tr}(H_{\mathrm{E},i} W) + \sigma_w^2\right)^N (N-1)!} \dd z\nonumber\\
		% 		& \quad \times \log \frac{\int_{T_{j-1,j}}^{T_{j,j+1}} \frac{z^{N-1} \exp \left(-\frac{z}{\mathrm{tr}(H_{\mathrm{E},i} W) + \sigma_w^2}\right)}{\left(\mathrm{tr}(H_{\mathrm{E},i} W) + \sigma_w^2\right)^N (N-1)!} \dd z}{\sum_{i' \in \mathcal{M^K}} \int_{T_{j-1,j}}^{T_{j,j+1}} \frac{z^{N-1} \exp \left(-\frac{z}{\mathrm{tr}(H_{\mathrm{E},i'} W) + \sigma_w^2}\right)}{\left(\mathrm{tr}(H_{\mathrm{E},i'} W) + \sigma_w^2\right)^N (N-1)!} \dd z},
		% 		\label{eq:foo}
		% 	\end{align}
		% 	and the mutual information can be expressed as a function of $W$ by combining \eqref{eq:backscatter_sum_rate} and \eqref{eq:foo}.

		% 	So far I have no idea how to solve this issue, and found no reference regarding precoder design for \gls{ambc}/\gls{sr} with discrete channels (although some naive combiner designs are available for \gls{bibo}). Personally, I believe the precoder design is the key to (i) boost the rate region and avoid blind spots in conventional \gls{ambc}; (ii) build our proposal over existing infrastructures. Ideally, assuming the number of transmit antennas $Q$ is larger than the number of tags $K$, the optimal energy levels should be almost uniformly spaced (as $z$ follows Erlang distribution) to concentrate the channel transitional probability on diagonal as possible.
		% \end{subsection}
	\end{section}
	\bibliographystyle{IEEEtran}
	\bibliography{IEEEabrv,library.bib}
\end{document}
